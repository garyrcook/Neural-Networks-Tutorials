{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial :- A Complete Practical Guide to Creating Neural Networks from the Mathematical Background to the Implementation with Python\n",
    "\n",
    "## By Gary Cook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "The aim of this tutorial and subsequent tutorials is to be a free guide to help anyone who wishes to learn about neural networks to develop a detailed and practical understanding of all the steps involved in building and training an artificial neural network.\n",
    "To that end, this introductory section is used to give an overview of the mathematical concepts that will be used in later sections and throughout those sections, each step will be explained in as much detail as possible and where appropriate demonstrated with diagrams, examples and code.\n",
    "\n",
    "Working with neural networks means working with matrices of numbers and matrix equation, so the first thing to be introduced in\n",
    "Section 0.1 is the concept of a matrix and the methods that will be used to manipulate matrices and understand matrix equations.\n",
    "In later sections, all calculations are given in concise matrix form, for those who are comfortable working with matrices, and in an expanded form, to help those who are less familiar with matrices to visualise the process and aid their understanding.\n",
    "\n",
    "An important method for training neural networks is the gradient descent algorithm and the values of the gradients used therein are calculated using the backpropagation algorithm and that algorithm involves passing values through a chain of partial derivatives, the equation of which is determined by the chain rule.\n",
    "Section 0.2 starts with an introduction to the concepts of derivatives and partial derivatives from a very limited perspective that focuses on understanding the concept of gradient.\n",
    "After a brief introduction to the ideas of local minimum points and convexity, the gradient descent algorithm is introduced, followed by an introduction to the chain rule that will be used to calculate the gradients.\n",
    "The gradient descent algorithm, its preliminaries and the chain rule are first introduced for functions in one variable and then extended to multivariable functions so as to gently introduce the reader to these concepts with the help of simple examples and code.\n",
    "\n",
    "One of the simplest types of neural network is the Feedforward Artificial Neural Network (ANN) and in Section 1 it is networks of this type that are introduced, explained and created from scratch with Python and NumPy.\n",
    "\n",
    "It is the authors intent that in later sections, at the very least, the concepts of Recurrent Neural Networks RNNS and Long Short-Term Memory (LSTM) will be introduced and created from scratch with Python and NumPy and possibly with TensorFlow.\n",
    "\n",
    "**Prerequisites :-** A basic understanding of mathematics and mathematical notation in addition to a basic understanding of Python and NumPy.  \n",
    "**Useful :-** A basic understanding of matrices and calculus would be of great benefit; however, the minimum knowledge required to follow Section 1 is given in Section 0.1 and Section 0.2.  \n",
    "Anyone who is familiar with these concepts can skip to Section 1.\n",
    "\n",
    "**Contents of Section 0**\n",
    "- Section 0.1 Matrix Requirements  \n",
    " - Section 0.1.1 Matrices\n",
    " - Section 0.1.2 Matrix Addition\n",
    " - Section 0.1.3 Scalar Multiplication\n",
    " - Section 0.1.4 Hadamard Multiplication\n",
    " - Section 0.1.5 Transposition\n",
    " - Section 0.1.6 Dot Product\n",
    " - Section 0.1.7 Matrix Multiplication\n",
    " - Section 0.1.8 Manipulating Transpositions\n",
    " - Section 0.1.9 Functions\n",
    " - Section 0.1.10 Example\n",
    "- Section 0.2 Calculus Requirements\n",
    " - Section 0.2.1 Derivatives in One Variable\n",
    " - Section 0.2.2 Partial Derivatives\n",
    " - Section 0.2.3 Local minimum and Convex Functions in One Variable\n",
    " - Section 0.2.4 Gradient Descent Algorithm in One Variable\n",
    " - Section 0.2.5 Multivariable Gradient Descent Algorithm\n",
    " - Section 0.2.6 The Chain Rule in One Variable\n",
    " - Section 0.2.7 Multivariable Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Matrix Requirements\n",
    "\n",
    "### 0.1.1 Matrices\n",
    "\n",
    "An $m \\times n$ matrix $U$ is an array of elements(for our purposes numbers) with $m$ rows and $n$ columns that is denoted as \n",
    "$$\n",
    "U\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "u_{0, 0} & u_{0, 1} & u_{0, 2} & \\dots & u_{0, n-1} \\\\\n",
    "u_{1, 0} & u_{1, 1} & u_{1, 2} & \\dots & u_{1, n-1} \\\\\n",
    "u_{2, 0} & u_{2, 1} & u_{2, 2} & \\dots & u_{2, n-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "u_{m-1, 0} & u_{m-1, 1} & u_{m-1, 2} & \\dots & u_{m-1, n-1}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "where the number in the $i$-th row and $j$-th column is $u_{i, j}$, sometimes denoted as $u_{i j}$, $U_{i,j}$, $U_{i j}$, $U[i,j]$, so long as the meaning is unambiguous.\n",
    "\n",
    "A row vector $\\bf{a}$ is a $1 \\times n$ matrix that is denoted as\n",
    "$$\n",
    "{\\bf{a}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_0 & a_1 & a_2 & \\dots & a_{n-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A column vector $\\bf{b}$ is a $m \\times 1$ matrix that is denoted as\n",
    "$$\n",
    "{\\bf{b}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "b_0 \\\\\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_{m-1}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 0.1.2 Matrix Addition\n",
    "\n",
    "Let $U$ and $V$ be the $m \\times n$ matrices\n",
    "$$\n",
    "U\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "u_{0, 0} & u_{0, 1} & u_{0, 2} & \\dots & u_{0, n-1} \\\\\n",
    "u_{1, 0} & u_{1, 1} & u_{1, 2} & \\dots & u_{1, n-1} \\\\\n",
    "u_{2, 0} & u_{2, 1} & u_{2, 2} & \\dots & u_{2, n-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "u_{m-1, 0} & u_{m-1, 1} & u_{m-1, 2} & \\dots & u_{m-1, n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "V\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "v_{0, 0} & v_{0, 1} & v_{0, 2} & \\dots & v_{0, n-1} \\\\\n",
    "v_{1, 0} & v_{1, 1} & v_{1, 2} & \\dots & v_{1, n-1} \\\\\n",
    "v_{2, 0} & v_{2, 1} & v_{2, 2} & \\dots & v_{2, n-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "v_{m-1, 0} & v_{m-1, 1} & v_{m-1, 2} & \\dots & v_{m-1, n-1}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "then their addition is performed element-wise; that is,\n",
    "$$\n",
    "U+V\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "u_{0, 0} + v_{0, 0} & u_{0, 1} + v_{0, 1} & u_{0, 2} + v_{0, 2} & \\dots & u_{01, n-1} + v_{0, n-1} \\\\\n",
    "u_{1, 0} + v_{1, 0} & u_{1, 1} + v_{1, 1} & u_{1, 2} + v_{1, 2} & \\dots & u_{1, n-1} + v_{1, n-1} \\\\\n",
    "u_{2, 0} + v_{2, 0} & u_{2, 1} + v_{2, 1} & u_{1, 2} + v_{2, 2} & \\dots & u_{2, n-1} + v_{2, n-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "u_{m-1, 0} + v_{m-1, 0} & u_{m-1, 1} + v_{m-1, 1} & u_{m-1, 2} + v_{m-1, 2} & \\dots & u_{m-1, n-1} + v_{m-1, n-1}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 0.1.3 Scalar Multiplication\n",
    "\n",
    "Let $c$ be a scalar(a number) and let $U$ be the $m \\times n$ matrix\n",
    "$$\n",
    "U\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "u_{0, 0} & u_{0, 1} & u_{0, 2} & \\dots & u_{0, n-1} \\\\\n",
    "u_{1, 0} & u_{1, 1} & u_{1, 2} & \\dots & u_{1, n-1} \\\\\n",
    "u_{2, 0} & u_{2, 1} & u_{2, 2} & \\dots & u_{2, n-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "u_{m-1, 0} & u_{m-1, 1} & u_{m-1, 2} & \\dots & u_{m-1, n-1}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "then scalar multiplication is element-wise; that is,\n",
    "$$\n",
    "c U\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "c u_{0, 0} & c u_{0, 1} & c u_{0, 2} & \\dots & c u_{0, n-1} \\\\\n",
    "c u_{1, 0} & c u_{1, 1} & c u_{1, 2} & \\dots & c u_{1, n-1} \\\\\n",
    "c u_{2, 0} & c u_{2, 1} & c u_{2, 2} & \\dots & c u_{2, n-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "c u_{m-1, 0} & c u_{m-1, 1} & c u_{m-1, 2} & \\dots & c u_{m-1, n-1}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 0.1.4 Hadamard Multiplication\n",
    "\n",
    "Let $U$ and $V$ be the $m \\times n$ matrices\n",
    "$$\n",
    "U\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "u_{0, 0} & u_{0, 1} & u_{0, 2} & \\dots & u_{0, n-1} \\\\\n",
    "u_{1, 0} & u_{1, 1} & u_{1, 2} & \\dots & u_{1, n-1} \\\\\n",
    "u_{2, 0} & u_{2, 1} & u_{2, 2} & \\dots & u_{2, n-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "u_{m-1, 0} & u_{m-1, 1} & u_{m-1, 2} & \\dots & u_{m-1, n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "V\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "v_{0, 0} & v_{0, 1} & v_{0, 2} & \\dots & v_{0, n-1} \\\\\n",
    "v_{1, 0} & v_{1, 1} & v_{1, 2} & \\dots & v_{1, n-1} \\\\\n",
    "v_{2, 0} & v_{2, 1} & v_{2, 2} & \\dots & v_{2, n-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "v_{m-1, 0} & v_{m-1, 1} & v_{m-1, 2} & \\dots & v_{m-1, n-1}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "then Hadamard multiplication is performed element-wise; that is,\n",
    "$$\n",
    "U \\circ V\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "u_{0, 0}v_{0, 0} & u_{0, 1}v_{0, 1} & u_{0, 2}v_{0, 2} & \\dots & u_{01, n-1}v_{0, n-1} \\\\\n",
    "u_{1, 0}v_{1, 0} & u_{1, 1}v_{1, 1} & u_{1, 2}v_{1, 2} & \\dots & u_{1, n-1}v_{1, n-1} \\\\\n",
    "u_{2, 0}v_{2, 0} & u_{2, 1}v_{2, 1} & u_{1, 2}v_{2, 2} & \\dots & u_{2, n-1}v_{2, n-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "u_{m-1, 0}v_{m-1, 0} & u_{m-1, 1}v_{m-1, 1} & u_{m-1, 2}v_{m-1, 2} & \\dots & u_{m-1, n-1}v_{m-1, n-1}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 0.1.5 Transposition\n",
    "\n",
    "Let $U$ be the $m \\times n$ matrix\n",
    "$$\n",
    "U\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "u_{0, 0} & u_{0, 1} & u_{0, 2} & \\dots & u_{0, n-1} \\\\\n",
    "u_{1, 0} & u_{1, 1} & u_{1, 2} & \\dots & u_{1, n-1} \\\\\n",
    "u_{2, 0} & u_{2, 1} & u_{2, 2} & \\dots & u_{2, n-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "u_{m-1, 0} & u_{m-1, 1} & u_{m-1, 2} & \\dots & u_{m-1, n-1}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "then its transpose, denoted $U^T$ is the $n \\times m$ matrix\n",
    "$$\n",
    "U^T\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "u_{0, 0} & u_{1, 0} & u_{2, 0} & \\dots & u_{m-1, 0} \\\\\n",
    "u_{0, 1} & u_{1, 1} & u_{2, 1} & \\dots & u_{m-1, 1} \\\\\n",
    "u_{0, 2} & u_{1, 2} & u_{2, 2} & \\dots & u_{m-1, 2} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "u_{0, n-1} & u_{1, n-1} & u_{2, n-1} & \\dots & u_{m-1, n-1}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "the $i$-th row and $j$-th column of which are the $i$-th column and $j$-the row of $U$.\n",
    "\n",
    "The transpose of the row vector \n",
    "$$\n",
    "{\\bf{a}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_0 & a_1 & a_2 & \\dots & a_{n-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "is the column vector\n",
    "$$\n",
    "{\\bf{a}}^T\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_0 \\\\\n",
    "a_1 \\\\\n",
    "a_2 \\\\\n",
    "\\vdots \\\\\n",
    "a_{n-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and the transpose of the column vector \n",
    "$$\n",
    "{\\bf{b}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "b_0 \\\\\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_{m-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "is the row vector\n",
    "$$\n",
    "{\\bf{b}}^T\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "b_0 & b_1 & b_2 & \\dots & b_{m-1}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 0.1.6 Dot Product\n",
    "\n",
    "Let $\\bf{a}$ and $\\bf{b}$ be the row vectors with $n$ elements\n",
    "$$\n",
    "{\\bf{a}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_0 & a_1 & a_2 & \\dots & a_{n-1}\n",
    "\\end{bmatrix}\n",
    "\\text{ and }\n",
    "{\\bf{b}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "b_0 & b_1 & b_2 & \\dots & b_{n-1}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "then the dot product of $\\bf{a}$ and $\\bf{b}$ is the sum of $a_i b_i$ for all $i = 0, 1, 2, \\ldots, n-1$; that is,\n",
    "$$\n",
    "{\\bf{a}} \\cdot {\\bf{b}} = \\sum_{i=0}^{n-1} a_i b_i = a_0 b_0 + a_1 b_1 + a_2 b_2 + \\dots + a_{n-1} b_{n-1}.\n",
    "$$\n",
    "\n",
    "Similarly, let $\\bf{a}$ and $\\bf{b}$ be the column vectors with $n$ elements\n",
    "$$\n",
    "{\\bf{a}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_0 \\\\\n",
    "a_1 \\\\\n",
    "a_2 \\\\\n",
    "\\vdots \\\\\n",
    "a_{n-1}\n",
    "\\end{bmatrix}\n",
    "\\text{ and }\n",
    "{\\bf{b}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "b_0 \\\\\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_{n-1}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "then the dot product of $\\bf{a}$ and $\\bf{b}$ is the sum of $a_i b_i$ for all $i = 0, 1, 2, \\ldots, n-1$; that is,\n",
    "$$\n",
    "{\\bf{a}} \\cdot {\\bf{b}} = \\sum_{i=0}^{n-1} a_i b_i = a_0 b_0 + a_1 b_1 + a_2 b_2 + \\dots + a_{n-1} b_{n-1}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 0.1.7 Matrix Multiplication\n",
    "\n",
    "Let $\\bf{a}$ be the row vector with $n$ elements\n",
    "$$\n",
    "{\\bf{a}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_0 & a_1 & a_2 & \\dots & a_{n-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and let $\\bf{b}$ be the column vector with $n$ elements\n",
    "$$\n",
    "{\\bf{b}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "b_0 \\\\\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_{n-1}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "then their matrix product, denoted $\\bf{ab}$, is the dot product of $\\bf{a}$ and ${\\bf{b}}^T$, or equivalently the dot product of ${\\bf{a}}^T$ and $\\bf{b}$; that is,\n",
    "$$\n",
    "{\\bf{ab}} = {\\bf{a}} \\cdot {\\bf{b}}^T = {\\bf{a}}^T \\cdot {\\bf{b}} = \\sum_{i=0}^{n-1} a_i b_i = a_0 b_0 + a_1 b_1 + a_2 b_2 + \\dots + a_{n-1} b_{n-1}.\n",
    "$$\n",
    "\n",
    "Let $U$ be the $m \\times p$ matrix\n",
    "$$\n",
    "U\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "u_{0, 0} & u_{0, 1} & u_{0, 2} & \\dots & u_{0, p-1} \\\\\n",
    "u_{1, 0} & u_{1, 1} & u_{1, 2} & \\dots & u_{1, p-1} \\\\\n",
    "u_{2, 0} & u_{2, 1} & u_{2, 2} & \\dots & u_{2, p-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "u_{m-1, 0} & u_{m-1, 1} & u_{m-1, 2} & \\dots & u_{m-1, p-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and let $V$ be the $p \\times n$ matrix\n",
    "$$\n",
    "V\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "v_{0, 0} & v_{0, 1} & v_{0, 2} & \\dots & v_{0, n-1} \\\\\n",
    "v_{1, 0} & v_{1, 1} & v_{1, 2} & \\dots & v_{1, n-1} \\\\\n",
    "v_{2, 0} & v_{2, 1} & v_{2, 2} & \\dots & v_{2, n-1} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\dots & \\vdots \\\\\n",
    "v_{p-1, 0} & v_{p-1, 1} & v_{p-1, 2} & \\dots & v_{p-1, n-1}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "then their matrix product, denoted $UV$, is the $m$ by $n$ matrix the element in the $j$-th row and $k$-th column of which is the matrix product of the $j$-th row of the matrix $U$ and the $k$-th column of the matrix $V$; that is, \n",
    "$$\n",
    "UV_{j k} = \\sum_{i = 0}^{p-1} u_{j i} v_{i k} =  u_{j 0} v_{0 k} + u_{j 1} v_{1 k} + u_{j 2} v_{2 k} + \\dots u_{j, p-1} v_{p-1, k},\n",
    "$$\n",
    "this is why the number of columns in $U$ is equal to the number of rows in $V$ and why $UV$ has the same number of rows as $U$ and the same number of columns as $V$.\n",
    "\n",
    "*Example*  \n",
    "Let $U$ be the $4 \\times 3$ matrix \n",
    "$$\n",
    "U\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "u_{0, 0} & u_{0, 1} & u_{0, 2}  \\\\\n",
    "u_{1, 0} & u_{1, 1} & u_{1, 2}  \\\\\n",
    "u_{2, 0} & u_{2, 1} & u_{2, 2}  \\\\\n",
    "u_{3, 0} & u_{3, 1} & u_{3, 2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and let $V$ be the $3 \\times 2$ matrix\n",
    "$$\n",
    "V\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "v_{0, 0} & v_{0, 1} \\\\\n",
    "v_{1, 0} & v_{1, 1} \\\\\n",
    "v_{2, 0} & v_{2, 1} \\\\\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "then $UV$ is the $4 \\times 2$ matrix \n",
    "$$\n",
    "UV\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "u_{0, 0} v_{0, 0} + u_{0, 1} v_{1, 0} + u_{0, 2} v_{2, 0} & u_{0, 0} v_{0, 1} + u_{0, 1} v_{1, 1} + u_{0, 2} v_{2, 1} \\\\\n",
    "u_{1, 0} v_{0, 0} + u_{1, 1} v_{1, 0} + u_{1, 2} v_{2, 0} & u_{1, 0} v_{0, 1} + u_{1, 1} v_{1, 1} + u_{1, 2} v_{2, 1} \\\\\n",
    "u_{2, 0} v_{0, 0} + u_{2, 1} v_{1, 0} + u_{2, 2} v_{2, 0} & u_{2, 0} v_{0, 1} + u_{2, 1} v_{1, 1} + u_{2, 2} v_{2, 1} \\\\\n",
    "u_{3, 0} v_{0, 0} + u_{3, 1} v_{1, 0} + u_{3, 2} v_{2, 0} & u_{3, 0} v_{0, 1} + u_{3, 1} v_{1, 1} + u_{3, 2} v_{2, 1}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 0.1.8 Manipulating Transpositions\n",
    "\n",
    "In Python, it is often easier to work with vectors in row form that are presented in column form in calculations, so it is necessary to know how to manipulate matrix equations through transposition.  \n",
    "The following simple rules are sufficient for that purpose.\n",
    "1. $(W^T)^T = W$.  \n",
    "2. If $W = U + V$, then $W^T = U^T + V^T$.  \n",
    "3. If $W = cU$, where $c$ is a scalar, then $W^T = cU^T$.  \n",
    "4. If $W = U \\circ V$, then $W^T = U^T \\circ V^T$.  \n",
    "5. IF $W = UV$, then $W^T = V^T U^T$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 0.1.9 Functions\n",
    "\n",
    "Let $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ be a function, such as $f(x) = x^2 + 1$, and let $U$ be an $m \\times n$ matrix, then throughout this document the application of $f$ to $U$ element-wise is denoted as $f(U)$.\n",
    "\n",
    "*Example*  \n",
    "If $f(x) = x^2 + 1$ and \n",
    "$$\n",
    "U\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0 & 2 \\\\\n",
    "-3 & 0.5\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "then\n",
    "$$\n",
    "f(U)\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "f(0) & f(2) \\\\\n",
    "f(-3) & f(0.5)\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 5 \\\\\n",
    "10 & 1.25\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "### 0.1.10 Example\n",
    "\n",
    "Examples of how to create and manipulate the vectors\n",
    "$$\n",
    "a = \n",
    "\\begin{bmatrix}\n",
    "2 & -1 & 3\n",
    "\\end{bmatrix}\n",
    "\\text{ and }\n",
    "b = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "4 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and the matrices\n",
    "$$\n",
    "U = \n",
    "\\begin{pmatrix}\n",
    "1 & 0 & -1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & -1 & 1\n",
    "\\end{pmatrix}\n",
    "\\text{ and }\n",
    "V = \n",
    "\\begin{pmatrix}\n",
    "2 & 4 & 0 \\\\\n",
    "-3 & 1 & -2 \\\\\n",
    "0 & -1 & 7\n",
    "\\end{pmatrix};\n",
    "$$\n",
    "finishing with an example of matrix multiplication with multiple vectors that works because the vectors are in axis 0 of the NumPy array, see example in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector a\n",
      "[ 2 -1  3]\n",
      "Shape of Vector a\n",
      "(3,)\n",
      "Transpose of Vector a\n",
      "[[ 2]\n",
      " [-1]\n",
      " [ 3]]\n",
      "\n",
      "[ 2 -1  3]  - .T doesn't generate an array of shape (3,1)\n",
      "\n",
      "[ 2 -1  3]  - .transpose doesn't generate an array of shape (3,1)\n",
      "\n",
      "Vector a\n",
      "[[ 2 -1  3]]\n",
      "Shape of Vector a\n",
      "(1, 3)\n",
      "\n",
      "Vector a\n",
      "[[ 2 -1  3]]\n",
      "Shape of Vector a\n",
      "(1, 3)\n",
      "\n",
      "Transpose of Vector a\n",
      "[[ 2]\n",
      " [-1]\n",
      " [ 3]]\n",
      "\n",
      "[[ 2]\n",
      " [-1]\n",
      " [ 3]]\n",
      "\n",
      "[[ 2]\n",
      " [-1]\n",
      " [ 3]]\n",
      "\n",
      "Vector b\n",
      "[[[0]\n",
      "  [4]\n",
      "  [3]]]\n",
      "Shape of Vector b\n",
      "(1, 3, 1)\n",
      "\n",
      "Vector b\n",
      "[[0]\n",
      " [4]\n",
      " [3]]\n",
      "Shape of Vector b\n",
      "(3, 1)\n",
      "\n",
      "Transpose of Vector b\n",
      "[[0 4 3]]\n",
      "\n",
      "[[0 4 3]]\n",
      "\n",
      "[[0 4 3]]\n",
      "\n",
      "Matrix U\n",
      "[[ 1  0 -1]\n",
      " [ 0  1  0]\n",
      " [ 1 -1  1]]\n",
      "Shape of Matrix U\n",
      "(3, 3)\n",
      "\n",
      "Matrix U\n",
      "[[ 1  0 -1]\n",
      " [ 0  1  0]\n",
      " [ 1 -1  1]]\n",
      "Shape of Matrix U\n",
      "(3, 3)\n",
      "\n",
      "Transpose of Matrix U\n",
      "[[ 1  0  1]\n",
      " [ 0  1 -1]\n",
      " [-1  0  1]]\n",
      "\n",
      "Transpose of Matrix U\n",
      "[[ 1  0  1]\n",
      " [ 0  1 -1]\n",
      " [-1  0  1]]\n",
      "\n",
      "10 a\n",
      "[[ 20 -10  30]]\n",
      "\n",
      "-b\n",
      "[[ 0]\n",
      " [-4]\n",
      " [-3]]\n",
      "\n",
      "2U\n",
      "[[ 2  0 -2]\n",
      " [ 0  2  0]\n",
      " [ 2 -2  2]]\n",
      "\n",
      "U+V\n",
      "[[ 3  4 -1]\n",
      " [-3  2 -2]\n",
      " [ 1 -2  8]]\n",
      "\n",
      "U*V (Hadamard)\n",
      "[[2 0 0]\n",
      " [0 1 0]\n",
      " [0 1 7]]\n",
      "\n",
      "a . b\n",
      "[[5]]\n",
      "\n",
      "a b (Matrix Multiplication)\n",
      "[[5]]\n",
      "\n",
      "b a (Matrix Multiplication)\n",
      "[[ 0  0  0]\n",
      " [ 8 -4 12]\n",
      " [ 6 -3  9]]\n",
      "\n",
      "a b (Matrix Multiplication)\n",
      "[[5]]\n",
      "\n",
      "b a (Matrix Multiplication)\n",
      "[[ 0  0  0]\n",
      " [ 8 -4 12]\n",
      " [ 6 -3  9]]\n",
      "\n",
      "U V (Matrix Multiplication)\n",
      "[[ 2  5 -7]\n",
      " [-3  1 -2]\n",
      " [ 5  2  9]]\n",
      "\n",
      "U V (Matrix Multiplication)\n",
      "[[ 2  5 -7]\n",
      " [-3  1 -2]\n",
      " [ 5  2  9]]\n",
      "\n",
      "e^U + 1\n",
      "[[3.71828183 2.         1.36787944]\n",
      " [2.         3.71828183 2.        ]\n",
      " [3.71828183 1.36787944 3.71828183]]\n",
      "\n",
      "[a U, b.T U]\n",
      "[[ 5 -4  1]\n",
      " [ 3  1  3]]\n",
      "\n",
      "[a.T U, b U]\n",
      "[[[ 5]\n",
      "  [-4]\n",
      "  [ 1]]\n",
      "\n",
      " [[ 3]\n",
      "  [ 1]\n",
      "  [ 3]]]\n",
      "\n",
      "[ 2 -1  3]\n",
      "[0 4 3]\n",
      "[[ 2]\n",
      " [-1]\n",
      " [ 3]]\n",
      "[[0]\n",
      " [4]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "# Creating vectors, matrices and their transposes\n",
    "\n",
    "print(\"Vector a\")\n",
    "vector_a = np.array([2, -1, 3])\n",
    "print(vector_a)\n",
    "print(\"Shape of Vector a\")\n",
    "print(vector_a.shape)\n",
    "print(\"Transpose of Vector a\")\n",
    "print(vector_a.reshape(3,1))\n",
    "print()\n",
    "print(vector_a.T, \" - .T doesn't generate an array of shape (3,1)\")\n",
    "print()\n",
    "print(np.transpose(vector_a), \" - .transpose doesn't generate an array of shape (3,1)\")\n",
    "print()\n",
    "print(\"Vector a\")\n",
    "vector_a = np.array([[2, -1, 3]])\n",
    "print(vector_a)\n",
    "print(\"Shape of Vector a\")\n",
    "print(vector_a.shape)\n",
    "print()\n",
    "print(\"Vector a\")\n",
    "vector_a = np.array([2, -1, 3]).reshape(1,3)\n",
    "print(vector_a)\n",
    "print(\"Shape of Vector a\")\n",
    "print(vector_a.shape)\n",
    "print()\n",
    "print(\"Transpose of Vector a\")\n",
    "print(vector_a.reshape(3,1))\n",
    "print()\n",
    "print(vector_a.T)\n",
    "print()\n",
    "print(np.transpose(vector_a))\n",
    "print()\n",
    "print(\"Vector b\")\n",
    "vector_b = np.array([[[0], [4], [3]]])\n",
    "print(vector_b)\n",
    "print(\"Shape of Vector b\")\n",
    "print(vector_b.shape)\n",
    "print()\n",
    "print(\"Vector b\")\n",
    "vector_b = np.array([0, 4, 3]).reshape(3,1)\n",
    "print(vector_b)\n",
    "print(\"Shape of Vector b\")\n",
    "print(vector_b.shape)\n",
    "print()\n",
    "print(\"Transpose of Vector b\")\n",
    "print(vector_b.reshape(1,3))\n",
    "print()\n",
    "print(vector_b.T)\n",
    "print()\n",
    "print(np.transpose(vector_b))\n",
    "print()\n",
    "print(\"Matrix U\")\n",
    "matrix_U = np.array([[1, 0, -1], [0, 1, 0], [1, -1, 1]])\n",
    "print(matrix_U)\n",
    "print(\"Shape of Matrix U\")\n",
    "print(matrix_U.shape)\n",
    "print()\n",
    "print(\"Matrix U\")\n",
    "matrix_U = np.array([1, 0, -1, 0, 1, 0, 1, -1, 1]).reshape(3,3)\n",
    "print(matrix_U)\n",
    "print(\"Shape of Matrix U\")\n",
    "print(matrix_U.shape)\n",
    "print()\n",
    "print(\"Transpose of Matrix U\")\n",
    "print(matrix_U.T)\n",
    "print()\n",
    "print(\"Transpose of Matrix U\")\n",
    "print(np.transpose(matrix_U))\n",
    "print()\n",
    "\n",
    "# Scalar Multiplcation\n",
    "print(\"10 a\")\n",
    "print(10*vector_a)\n",
    "print()\n",
    "print(\"-b\")\n",
    "print(-1*vector_b)\n",
    "print()\n",
    "print(\"2U\")\n",
    "print(2*matrix_U)\n",
    "print()\n",
    "\n",
    "# Matrix Addition\n",
    "matrix_V = np.array([2, 4, 0, -3, 1, -2, 0, -1, 7]).reshape(3,3)\n",
    "print(\"U+V\")\n",
    "print(matrix_U + matrix_V)\n",
    "print()\n",
    "\n",
    "# Hadamard Multiplication\n",
    "print(\"U*V (Hadamard)\")\n",
    "print(matrix_U * matrix_V)\n",
    "print()\n",
    "\n",
    "# Dot Product\n",
    "print(\"a . b\")\n",
    "print(np.dot(vector_a,vector_b))\n",
    "print()\n",
    "print(\"a b (Matrix Multiplication)\")\n",
    "print(np.dot(vector_a, vector_b))\n",
    "print()\n",
    "print(\"b a (Matrix Multiplication)\")\n",
    "print(np.dot(vector_b, vector_a))\n",
    "print()\n",
    "print(\"a b (Matrix Multiplication)\")\n",
    "print(np.matmul(vector_a, vector_b))\n",
    "print()\n",
    "print(\"b a (Matrix Multiplication)\")\n",
    "print(np.matmul(vector_b, vector_a))\n",
    "print()\n",
    "\n",
    "# Matrix Multiplication\n",
    "print(\"U V (Matrix Multiplication)\")\n",
    "print(np.matmul(matrix_U, matrix_V))\n",
    "print()\n",
    "print(\"U V (Matrix Multiplication)\")\n",
    "print(np.dot(matrix_U, matrix_V))\n",
    "print()\n",
    "\n",
    "# Functions\n",
    "print(\"e^U + 1\")\n",
    "print(np.exp(matrix_U) + 1)\n",
    "print()\n",
    "\n",
    "# Operations on Multiple Vectors\n",
    "print(\"[a U, b.T U]\")\n",
    "vectors_a_and_b = np.array([[2, -1, 3], [0, 4, 3]])\n",
    "print(np.matmul(vectors_a_and_b, matrix_U))\n",
    "print()\n",
    "\n",
    "print(\"[a.T U, b U]\")\n",
    "vectors_a_and_b_transpose = np.array([[[2], [-1], [3]], [[0], [4], [3]]])\n",
    "print(np.matmul(matrix_U.T, vectors_a_and_b_transpose))\n",
    "print()\n",
    "\n",
    "# Axis 0\n",
    "for i in vectors_a_and_b:\n",
    "    print(i)\n",
    "for i in vectors_a_and_b_transpose:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Calculus Requirements\n",
    "\n",
    "### 0.2.1 Derivatives in One Variable\n",
    "\n",
    "The derivative of the function $y = f(x)$, if it exists, is a function, denoted $\\frac{dy}{dx}$ or $f'(x)$, the value of which at $x=a$, denoted $\\frac{dy}{dx} \\Big|_a$ or $f'(a)$, is the slope (or gradient) of the curve described by the equation $y = f(x)$ at the point $(a, f(a))$.\n",
    "\n",
    "To visualise the gradient of $f$ at a point $(a,f(a))$ it helps to plot segments of the tangent line to $f$ at that point, that is, the line with gradient $f'(a)$ that passes through the point $(a, f(a))$ or algebraically the line given by the equation \n",
    "$y - f(a) = f'(a) (x - a)$.\n",
    "\n",
    "For example, let $f$ be the function $f(x) = x^2 + 1$, then its derivative is $f'(x) = 2x$, so the gradient of the curve \n",
    "$y = x^2+1$ at $x=-4$, $x=-1$, $x=0$ and $x=1$, $x=4$ is $f'(-4) = -8$, $f'(-1) = -2$, $f'(0) = 0$ $f'(1) = 2$ and $f'(4) = 8$ respectively.\n",
    "\n",
    "The tangent line to $f$ at $x=-4$ is $y= -8x - 15$, the tangent line to $f$ at $x = -2$ is $y = -4x - 3$, the tangent line to $f$ at $x=0$ is $y = 1$, the tangent line to $f$ at $x=2$ is $y = 4x - 3$ and the tangent line to $f$ at $x=4$ is $y = 8x - 15$.\n",
    "\n",
    "**Note:-** Calculating the derivative of a function is not discussed here. The derivatives of only a small number of functions are required here and these are given explicitly. The derivatives of the three activation functions that are given in Section 1.1 are given in Section 1.6.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# the curve x**2 + 1\n",
    "x = np.linspace(-8, 8, 100)\n",
    "y = eval('x**2 + 1')\n",
    "\n",
    "# tangent line at x = 4\n",
    "x0 = np.linspace(2, 6, 100)\n",
    "t0 = eval('8*x0 - 15')\n",
    "\n",
    "# tangent line at x = 2\n",
    "x1 = np.linspace(0, 4, 100)\n",
    "t1 = eval('4*x1 - 3')\n",
    "\n",
    "# tangent line at x = 0\n",
    "x2 = np.linspace(-2, 2, 100)\n",
    "t2 = eval('0*x2 + 1')\n",
    "\n",
    "# tangent line at x = -2\n",
    "x3 = np.linspace(-4, 0, 100)\n",
    "t3 = eval('-4*x3 -3')\n",
    "\n",
    "# tangent line at x = -4\n",
    "x4 = np.linspace(-6, -2, 100)\n",
    "t4 = eval('-8*x4 -15')\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x0, t0)\n",
    "plt.plot(x1 ,t1)\n",
    "plt.plot(x2 ,t2)\n",
    "plt.plot(x3 ,t3)\n",
    "plt.plot(x4 ,t4)\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.2 Partial Derivatives\n",
    "\n",
    "A continuous function in one variable $y = f(x)$ receives a value of $x$ and for every value of $x$ there exists exactly one value of $y$ that satisfies $y=f(x)$ and the points $(x,y=f(x))$ exist in the $xy$-plane with a horizontal $x$-axis and a vertical $y$-axis, which can be visualised, for example, in the depiction of $y = x^2 +1$ above.\n",
    "\n",
    "A continuous function in $n$ variables $y = f(x_0, x_1, \\ldots, x_{n-1})$ receives $n$ values, one for every variable, and for every set of values of $(x_0, x_1, \\ldots, x_{n-1})$ there exists exactly one value of $y$ that satisfies \n",
    "$y = f(x_0, x_1, \\ldots, x_{n-1})$ and the points $(x_0, x_1, \\ldots, x_{n-1}, y=f(x_0, x_1, \\ldots, x_{n-1})$ exist in $(n+1)$-dimensional space with an $x_0$-axis, $x_1$-axis, $\\ldots$ $x_{n-1}$-axis and $y$-axis.\n",
    "\n",
    "If $f$ receives only values of the form $(0, 0, \\ldots, 0, x_i, 0, \\ldots, 0)$, then the points $(0, 0, \\ldots, 0, x_i, 0, \\ldots, 0, y)$ exist in the $x_i y$-plane with an $x_i$-axis and a $y$-axis.\n",
    "In fact, $y = g(x_i) = f(0, 0, \\ldots, 0, x_i, 0, \\ldots, 0)$ is a function in one variable in the $x_i y$-plane.\n",
    "\n",
    "Generalising, if $f$ only receives values of the form  \n",
    "\n",
    "$$\n",
    "(a_0, a_1, \\ldots, a_{i-1}, x_i, a_{i+1}, \\ldots, a_{n-1}),\n",
    "$$\n",
    "\n",
    "where $a_0, a_1, \\ldots, a_{n-1}$ are constants, then the points  \n",
    "\n",
    "$$\n",
    "(a_0, a_1, \\ldots, a_{i-1}, x_i, a_{i+1}, \\ldots, a_{n-1}, y = f(a_0, a_1, \\ldots, a_{i-1}, x_i, a_{i+1}, \\ldots, a_{n-1}))\n",
    "$$\n",
    "\n",
    "exist in a plane $P$ that is parallel to the $x_i y$-plane and passes through the point  \n",
    "\n",
    "$$\n",
    "(a_0, a_1, \\ldots, a_{i-1}, 0, a_{i+1}, \\ldots, a_{n-1}, y = f(a_0, a_1, \\ldots, a_{i-1}, 0, a_{i+1}, \\ldots, a_{n-1})).\n",
    "$$\n",
    "\n",
    "The plane $P$ is parallel to the $x_i y$-axis, so if $y = g(x_i)$, where\n",
    "$g(x_i) = f(a_0, a_1, \\ldots, a_{i-1}, x_i, a_{i+1}, \\ldots, a_{n-1})$, is a function in one variable in the $x_i y$-plane, then the gradient of $g$ at $a_i$ is the gradient of $f$ at $a = (a_0, a_1, \\ldots, a_{n-1})$ along the $x_i$-axis.\n",
    "\n",
    "*Partial Derivative*\n",
    "\n",
    "The partial derivative of $f$ with respect to $x_i$ is a function, denoted, $\\frac{\\partial y}{\\partial x_i}$ or $f_{x_i}$ the value of which at $a = (a_0, a_1, \\ldots, a_{n-1})$ is the derivative of the function $y = g(x_i)$ at $a_i$, denoted $\\frac{\\partial y}{\\partial x_i} \\bigg|_a$ or $f_{x_i}(a)$.\n",
    "\n",
    "That is, the partial derivative of $f$ with respect to $x_i$ at $a$ is the slope (or gradient) of the curve described by $f$ along the $x_i$-axis at the point $(a, f(a))$.\n",
    "\n",
    "*Example*\n",
    "\n",
    "Consider the function $y = f(u, v, w, x) = w(u^4 + x^2 + 1) + v + 1$ at $(0, -1, 1, 4)$.\n",
    "If $f$ only receives values of the form $(0, -1, 1, x)$, then $y = g(x) = f(0, -1, 1, x) = x^2 + 1$ and $g'(x) = 2x$; hence, \n",
    "$f_x(0,-1,1,4) = g'(4) = 8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.3 Local Minima and Convex Functions in One Variable\n",
    "\n",
    "*Local Minima*\n",
    "\n",
    "A local minimum (or local minimum point) of a function $y = f(x)$ is a point \n",
    "$(m,f(m))$ that satisfies $f(x) \\geq f(m)$ for all $x$ in some neighbourhood of $m$.\n",
    "At such a point $\\frac{dy}{dx} \\bigg|_m = 0$.\n",
    "A local minimum is a global minimum (or global minimum point) if the condition is true for all $x \\in \\mathbb{R}$.\n",
    "\n",
    "*Convex Functions*\n",
    "\n",
    "A function $y = f(x)$ is (strictly) locally convex in an open region $B = (b_0, b_1)$ if \n",
    "$$\n",
    "f(t x_1 + (1-t) x_2) \\leq (<) \\ t f(x_1) + (1-t) f(x_2)\n",
    "$$\n",
    "for all points $x_1, x_2 \\in B$ and for all $t \\in [0,1]$. The function $f(x)$ is (strictly) globally convex if the condition is true for all $x_1, x_2 \\in \\mathbb{R}$.\n",
    "\n",
    "*Explanation*\n",
    "\n",
    "Simply put, strictly convex means that if a line is drawn from $(x_1, f(x_1))$ to $(x_2, f(x_2))$, then $f(x)$ is below that line for all $x$ in the region $(x_1, x_2)$ and removing the (strict) condition allows contact between $f$ and the line in that region, which would allow a plateau.  \n",
    "\n",
    "*Examples*\n",
    "\n",
    "- In the example depicted above the point at $x=0$ is shown to be a local minimum of the function $f(x) = x^2 + 1$; in fact, it is a global minimum.\n",
    "\n",
    "> In addition, $f(x)$ is globally convex in a neighbourhood of $x=0$.\n",
    "\n",
    "- The function $f(x) = 3x^4 - 6 x^2 + 2$ that is depicted below has two local minima at $x=-1$ and $x=1$; in fact, since they are the only local minima and $f(-1) = f(1) = -1$ they are global minima.\n",
    "\n",
    "> In addition, $f(x)$ is locally convex in a neighbourhood of both $x=-1$ and $x=1$.\n",
    "\n",
    "- The function $f(x) = 3x^4 - 16x^3 + 18x^2$ that is depicted below has two local minima at $x=0$ and at $x=3$; in fact since they are the only local minima it follows that the point at $x=3$ is a global minimum.\n",
    "\n",
    "> In addition, $f(x)$ is locally convex in a neighbourhood of both $x=0$ and $x=3$.\n",
    "\n",
    "- The function $f(x) = -x^2 - 1$ that is depicted below has no local minima.\n",
    "\n",
    "> In addition, $f(x)$ is not locally convex anywhere.\n",
    "\n",
    "- The function $f(x) = x^3$ that is depicted below has no local minima, only a saddle point at $x=0$; that is, a point that satisfies $\\frac{dy}{dx} \\bigg|_{\\bar{x}} = 0$, but is not a local minimum or a local maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-1.5, 1.5, 100)\n",
    "y = eval('3*x**4 - 6*x**2 + 2')\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-1, 4, 100)\n",
    "y = eval('3*x**4 - 16*x**3 + 18*x**2')\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "y = eval('-1*x**2 -1')\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-3, 3, 100)\n",
    "y = eval('x**3')\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.4 Gradient Descent Algorithm in One Variable\n",
    "\n",
    "The gradient descent algorithm is an algorithm for finding the local minima of a differentiable function $y = f(x)$.\n",
    "\n",
    "## Algorithm: Gradient Descent\n",
    "\n",
    "> 1. Initiate the algorithm with a guess $x_0$ as to the local minimum.\n",
    "\n",
    "${\\bf{\\text{for }}} i = 0, 1, 2 \\ldots {\\bf{\\text{ do:}}}$\n",
    "\n",
    "> 2. Calculate $f'(x_i)$.\n",
    "\n",
    "> 3. Generate the next point in the sequence $x_{i+1} = x_i - \\eta f'(x_i)$.\n",
    "\n",
    "The hope is that the sequence $x_0, x_1, x_2, \\ldots$ converges to a local minimum.\n",
    "\n",
    "*The gradient descent is a simple algorithm that utilises the following facts.*\n",
    "\n",
    "- If $f$ is strictly locally convex in the neighbourhood $B = (b_0, b_1)$ and $f'(b_0) < 0 < f'(b_1)$, then $f$ has exactly one local minimum $m$ in $B$, conversely, if $f$ has an isolated local minimum at $m$, then $f$ is strictly locally convex in some neighbourhood $B$ of $m$.  \n",
    "- In $B$, on the right-hand-side of $m$, the shift $-f'(x_i)$ is negative, that is, in the direction of $m$ and the nearer a point is to $m$, the smaller the gradient of $f$ at that point.  \n",
    "- In $B$, on the left-hand-side of $m$, the shift $-f'(x_i)$ is positive, that is, in the direction of $m$ and the nearer a point is to $m$, the greater the gradient of $f$ at that point.\n",
    "\n",
    "Remember that the gradient of $f$ at $m$ is zero.\n",
    "\n",
    "These facts are visualisable in the above depiction of the function $f(x) = x^2 + 1$ with its tangent lines.\n",
    "\n",
    "*Conditions under which the sequence generated by the gradient descent algorithm always converges.*\n",
    "\n",
    "If $f$ is strictly locally convex and $f'$ is locally Lipschitz continuous (see aside) in some open region $B = (b_0, b_1)$, where $f'(b_0) < 0 < f'(b_1)$, then, provided that $x_0 \\in B$ and that the constant $\\eta$ is sufficiently small, the sequence $x_0, x_1, x_2, \\ldots$ converges to the unique local minimum $m$ contained in $B$ from the direction of $x_0$.\n",
    "\n",
    "*Why?*\n",
    "\n",
    "Assume without loss of generality that $m = 0$ and $x_0 > 0$ and that $\\eta$ is sufficiently small to give $x_1 > 0$.\n",
    "If $x_i > 0$, then $f'(x_i) > 0$ and by local Lipschitz continuity \n",
    "$$\n",
    "|f'(x_i)| \\leq K|x_i|; \\text{ in fact } 0 < \\frac{f'(x_i)}{K(x_i)} \\leq 1.\n",
    "$$\n",
    "If $\\eta < \\frac{1}{K}$, then $0 < \\eta \\frac{f'(x_i)}{x_i} < 1$ and \n",
    "$$\n",
    "x_{i+1} = x_i(1 - \\eta \\frac{f'(x_i)}{x_i});\n",
    "$$\n",
    "hence, $0 < x_{i+1} < x_i$ meaning that $x_i$ converges to $a \\in [0, x_0)$, but if $a \\neq 0$ then $x_i$ converges to both $a$ and $a - \\eta f'(a)$ but $a - \\eta f'(a) < a$, a contradiction.\n",
    "\n",
    "*However!*\n",
    "\n",
    "Convergence can still occur if these conditions are not satisfied!\n",
    "\n",
    "**Note** :- The gradient descent algorithm can be applied to any function for which the derivative at a point is calculable (even a function without a local minimum), so it is no surprise that convergence is a hope and not a certainty (even if $f$ is perfectly suited to gradient descent, suitable values of $x_0$ and $\\eta$ are required).\n",
    "The sequence can converge, oscillate or diverge.\n",
    "Even if the sequence converges to a point, there is a possibility that it is a saddle point or a local minimum that is globally comparatively large.\n",
    "\n",
    "*Examples*\n",
    "\n",
    "- The function $f(x) = x^2 + 1$ is globally strictly convex and its derivative $f'(x) = 2x$ is locally Lipschitz continuous, so it is a perfect function to which to apply the gradient descent algorithm, however, the value $x_0$ and the constant $\\eta$ are still important, as can be seen in the output of the code below.\n",
    "Starting from $x_0 = 1$ if $\\eta = 0.001$ the convergence is slow, if $\\eta = 1$ the sequence oscillates, and if $\\eta = 0.1$ the convergence is good.\n",
    "\n",
    "- The function $f(x) = 3x^4 - 6x^2 + 2$ is strictly convex in a neighbourhood $B_0$ of the points $x=1$ and its derivative $f'(x) = 12x^3 - 12x$ is locally Lipschitz continuous in a neighbourhood $B1$ of $x=1$, so if \n",
    "$x_0$ is in $B_0 \\cap B1$ and $\\eta$ is sufficiently small, then the sequence converges to the local minimum at $x=1$; similarly, for the local minimum at $x=-1$.\n",
    "Starting from $x_0 = 2$, if $\\eta = 0.1$ the sequence diverges, and if $\\eta = 0.01$ the sequence converges.\n",
    "Starting from $x_0 = 0.1$ (not inside $B$), if $\\eta = 0.01$ the sequence still converges.\n",
    "\n",
    "- The function $f(x) = x^3$ is strictly convex and its derivative $f'(x) = 3x^2$ is locally Lipschitz continuous in the region $(0, -\\infty)$, so the sequence generated by the gradient descent algorithm will still converge for $x_0=1$ and $\\eta = 0.1$; however, it will converge to the point at $x=0$, which is a saddle point, not a local minimum, as can be seen in the code below.\n",
    "\n",
    "**Aside** :- *Locally Lipschitz Continuous*\n",
    "\n",
    "A function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ given by $y = f(x)$ is locally Lipschitz continuous in some open region $B = (b_0, b_1)$ if there exists a non-negative constant $K$ such that $|f(x_1) - f(x_2)| \\leq K |x_1 - x_2|$ for all $x_1, x_2 \\in B$.\n",
    "The function is globally Lipschitz continuous if this condition is true for all $x_1, x_2 \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(x, eta, expression, iterations):\n",
    "    for i in range(iterations):\n",
    "        dy_dx = eval(expression)\n",
    "        x -= eta*dy_dx\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998\n",
      "0.996004\n",
      "0.994011992\n",
      "0.992023968016\n",
      "0.990039920079968\n",
      "0.9880598402398081\n",
      "0.9860837205593285\n",
      "0.9841115531182099\n",
      "0.9821433300119734\n",
      "0.9801790433519495\n",
      "\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "\n",
      "0.8\n",
      "0.64\n",
      "0.512\n",
      "0.4096\n",
      "0.32768\n",
      "0.26214400000000004\n",
      "0.20971520000000005\n",
      "0.16777216000000003\n",
      "0.13421772800000004\n",
      "0.10737418240000003\n"
     ]
    }
   ],
   "source": [
    "# f(x) = x**2 + 1\n",
    "# f'(x) = 2*x\n",
    "\n",
    "GD(1, 0.001, '2*x', 10)\n",
    "print()\n",
    "GD(1, 1, '2*x', 5)\n",
    "print()\n",
    "GD(1, 0.1, '2*x', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.2\n",
      "157.2896\n",
      "-4669271.055749896\n",
      "1.221598536326226e+20\n",
      "-2.1875941691447127e+60\n",
      "\n",
      "1.28\n",
      "1.18194176\n",
      "1.125635994257337\n",
      "1.0895629998359264\n",
      "1.065093916852137\n",
      "1.0479128802695807\n",
      "1.0355741582199356\n",
      "1.026575370403076\n",
      "1.0199407791303592\n",
      "1.0150108921596146\n",
      "1.0113267544803675\n",
      "1.0085619726925235\n",
      "1.0064806332721277\n",
      "1.0049101291267692\n",
      "1.0037230045582364\n",
      "1.002824487397159\n",
      "1.002143735735421\n",
      "1.0016275835596637\n",
      "1.001236009337795\n",
      "1.0009388168912612\n",
      "\n",
      "0.11188000000000001\n",
      "0.12513754996039936\n",
      "0.1399189063854081\n",
      "0.15638046701682645\n",
      "0.17468721174647167\n",
      "0.1950099944865167\n",
      "0.2175212720033604\n",
      "0.24238876921579666\n",
      "0.269766513350266\n",
      "0.2997826572785043\n",
      "0.33252361295564453\n",
      "0.36801431226704107\n",
      "0.4061950081111457\n",
      "0.4468960216368737\n",
      "0.48981326702623973\n",
      "0.5344891133705623\n",
      "0.5803047539906234\n",
      "0.6264909581465223\n",
      "0.6721628315718864\n",
      "0.7163801596179774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# f(x) = 3*x**4 - 6*x**2 + 2, \n",
    "# f'(x) = 12*x**3 - 12*x\n",
    "\n",
    "GD(2, 0.1, '12*x**3 - 12*x', 5)\n",
    "print()\n",
    "GD(2, 0.01, '12*x**3 - 12*x', 20)\n",
    "print()\n",
    "GD(0.1, 0.01, '12*x**3 - 12*x', 20)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5529999999999999\n",
      "0.4612573\n",
      "0.397429810959013\n",
      "0.3500446745673379\n",
      "0.31328529230945185\n",
      "0.28384098999622626\n",
      "0.25967127771561493\n",
      "0.23944252597447693\n",
      "0.22224270900096552\n",
      "0.20742516248973916\n",
      "0.19451760307957075\n",
      "0.18316647370722433\n",
      "0.17310148658012253\n",
      "0.16411224918324802\n",
      "0.15603240008365266\n",
      "0.14872856712089313\n",
      "0.14209251111754292\n",
      "0.1360354266028362\n",
      "0.13048373541553152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# f(x) = x**3, \n",
    "# f'(x) = 3*x**2\n",
    "\n",
    "GD(1, 0.1, '3*x**2', 20)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.5 Multivariable Gradient Descent Algorithm\n",
    "\n",
    "As with a function in one variable a local minimum (or local minimum point) is a point \n",
    "$({\\bf{a}} = (a_0, a_1, \\ldots, a_{n-1}), f({\\bf{a}}))$ of a function \n",
    "$y = f({\\bf{x}} = (x_0, x_1, \\ldots, x_{n-1}))$ that satisfies $f({\\bf{a}}) \\leq f({\\bf{x}})$ for all ${\\bf{x}}$ in some neighbourhood of ${\\bf{a}}$.\n",
    "\n",
    "At such a point $\\frac{\\partial y}{\\partial x_i} \\bigg|_{\\bf{a}} = 0$ for all $i = 0,1,\\ldots$, so at the point $(\\bf{a},f({\\bf{a}}))$ the gradient of $f$ is zero along each of the $n$ axes, as it is in the one variable case.\n",
    "\n",
    "If the function $y = f({\\bf{x}})$ is differentiable, then the gradient of $f$, denoted $\\nabla f$, is the column vector\n",
    "$$\n",
    "\\nabla f \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial x_0} \\\\\n",
    "\\frac{\\partial y}{\\partial x_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial y}{\\partial x_{n-1}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and at a point $({\\bf{a}}, f({\\bf{a}}))$ it is the vector the elements of which are the gradients of $f$ at $({\\bf{a}}, f({\\bf{a}}))$ along the $n$ axes; that is,\n",
    "$$\n",
    "\\nabla f ({\\bf{a}})\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial x_0} ({\\bf{a}}) \\\\\n",
    "\\frac{\\partial y}{\\partial x_1} ({\\bf{a}}) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial y}{\\partial x_{n-1}} ({\\bf{a}})\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The multivariable version of the gradient is as follows.\n",
    "\n",
    "## Algorithm: Gradient Descent\n",
    "\n",
    "> 1. Initiate the algorithm with a guess ${\\bf{x}}_0$ as to the local minimum.\n",
    "\n",
    "${\\bf{\\text{for }}} j = 0, 1, 2 \\ldots {\\bf{\\text{ do:}}}$\n",
    "\n",
    "> 2. Calculate $\\nabla({\\bf{x}}_j)$.\n",
    "\n",
    "> 3. Generate the next point in the sequence ${\\bf{x}}_{j+1} = {\\bf{x}}_j - \\eta \\nabla ({\\bf{x}}_j)$.\n",
    "\n",
    "The hope is that the sequence ${\\bf{x}}_0, {\\bf{x}}_1, {\\bf{x}}_2, \\ldots$ converges to a local minimum.\n",
    "\n",
    "*Conditions under which the sequence generated by the gradient descent algorithm always converges.*\n",
    "\n",
    "There are multivariable versions of convexity and Lipschitz continuity and as with the one variable case if $y = f({\\bf{x}})$ is strictly convex and $\\nabla f$ is Lipschitz continuous in some open region $B = \\times_{i=0}^{n-1} B_i$, then, provided that \n",
    "${\\bf{x}}_0 \\in B$ and $\\eta$ is sufficiently small, the sequence ${\\bf{x}}_0, {\\bf{x}}_1, {\\bf{x}}_2, \\ldots$ converges to the unique local minimum ${\\bf{m}}$ in $B$ from the direction of ${\\bf{x}}_0$.\n",
    "\n",
    "*The reason is beyond the level of mathematics that is assumed and would require far more detail.*  \n",
    "...\n",
    "\n",
    "**Note:-**\n",
    "As with the one variable case convergence is still a hope in the multivariable case, because everything that can go wrong in the former can go wrong in the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.6 Chain Rule in One Variable\n",
    "\n",
    "The function $y = (f \\circ g)(x)$ defined by $y = f(g(x))$ is the composite of two functions, $y = f(z)$ and $z = g(x)$.\n",
    "The chain rule states that the derivate of $y = (f \\circ g)(x)$ is \n",
    "$$\n",
    "(f \\circ g)'(x) = g'(x)f'(g(x));\n",
    "$$\n",
    "that is,\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{dz} \\cdot \\frac{dz}{dx},\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{dz} \\bigg|_{z(x)} \\cdot \\frac{dz}{dx},\n",
    "$$\n",
    "in terms of $x$, note $y(x)$ is $y$ in terms of $x$.\n",
    "\n",
    "*Example*\n",
    "\n",
    "If $y = f(z) = z^4 + 1$ and $z = g(x) = (x^2+1)$, then $\\frac{dy}{dz} = 4z^3$, $\\frac{dz}{dx} = 2x$ and by the chain rule\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dy}{dx} &= \\frac{dy}{dz} \\cdot \\frac{dz}{dx} \\\\\n",
    "&= 4z^3 \\cdot 2x \\\\\n",
    "&= 4(x^2+1)^3 \\cdot 2x = 8x(x^2+1)^3.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*Repeated application of the chain rule*\n",
    "\n",
    "If $y = f \\circ (g \\circ h) = f(g(h(x)))$ is the composite of three functions $y = f(u)$, $u = g(v)$ and $v = h(x)$, then to calculate $\\frac{dy}{dx}$ apply the chain rule twice, as follows: by the chain rule\n",
    "$$\n",
    "\\frac{dy}{dv} = \\frac{du}{dv} \\cdot \\frac{dy}{du}\n",
    "$$\n",
    "and as $y = f(g(v))$ in terms of $v$ and $v = h(x)$ then by the chain rule\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dy}{dx} &= \\frac{dv}{dx} \\cdot \\frac{dy}{dv} \\\\\n",
    "&= \\frac{dv}{dx} \\cdot \\frac{du}{dv} \\cdot \\frac{dy}{du}\n",
    "\\end{align}\n",
    "$$\n",
    "or \n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dv}{dx} \\cdot \\frac{du}{dv} \\bigg|_{v(x)} \\cdot \\frac{dy}{du} \\bigg|_{u(x)}\n",
    "$$\n",
    "in terms of $x$, where $v(x)$ and $u(x)$ are $u$ and $v$ in terms of $x$, that is, $h(x)$ and $g(h(x))$.\n",
    "\n",
    "In general if \n",
    "$y = f_0 \\circ (f_1 \\circ (f_2 \\circ \\cdots (f_{n-2} \\circ f_{n-1}) \\cdots ))(x) =\n",
    "f_0(f_1(f_2( \\cdots f_{n-2}(f_{n-1}(x) \\cdots ))$\n",
    "is the composite of $n$ functions $y = f_0(u_0)$, $u_0 = f_1(u_1)$, $u_1 = f_2(u_2)$, $\\ldots$, $u_{n-2} = f_{n-1}(x)$, then applying the chain rule $n-1$ times and substituting in $u_i(x)$, for all $i = 0,1,\\ldots,n-2$, gives\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{du_{n-2}}{dx} \\cdot \\frac{du_{n-3}}{du_{n-2}} \\bigg|_{u_{n-2}(x)} \\cdot \\ \\cdots \\ \\cdot \\frac{du_0}{du_1} \\bigg|_{u_1(x)} \\cdot \\frac{dy}{du_0} \\bigg|_{u_0(x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.7 Multivariable Chain Rule\n",
    "\n",
    "For the composite function $y = f(z_0, z_1, \\ldots z_{m-1})$, where $z_i = g_i(x)$ for all $i = 0, 1, \\ldots, m-1$, the multivariable chain rule states that\n",
    "$$\n",
    "\\frac{dy}{dx} = \\sum_{i=0}^{m-1} \\frac{d z_i}{dx} \\cdot \\frac{\\partial y}{\\partial z_i}\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\frac{dy}{dx} = \\sum_{i=0}^{m-1} \\frac{d z_i}{dx} \\cdot \\frac{\\partial y}{\\partial z_i} \\bigg|_{z_i(x)}\n",
    "$$\n",
    "in terms of $x$.\n",
    "\n",
    "If $z_i = g_i(x)$ is replaced with $z_i = g_i(x_0, x_1, \\ldots, x_{n-1})$, then\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_j} = \\sum_{i=0}^{m-1} \\frac{\\partial z_i}{\\partial x_j} \\cdot \\frac{\\partial y}{\\partial z_i}\n",
    "$$\n",
    "for all $j = 0, 1, \\ldots, n-1$.\n",
    "\n",
    "*Repeating the multivariable chain rule*\n",
    "\n",
    "Generalise the notation, and consider the composite function \n",
    "$$\n",
    "y = f(x_0^{(0)}, x_1^{(0)}, \\ldots, x_{n_0-1}^{(0)}),\n",
    "$$\n",
    "where \n",
    "$$\n",
    "x_{i_0}^{(0)} = f_{i_0}^{(0)}(x_0^{(1)}, x_1^{(1)}, \\ldots, x_{n_1-1}^{(1)})\n",
    "$$\n",
    "for all $i_0 = 0, 1, \\ldots, n_0-1$.  \n",
    "By the multivariable chain rule the partial derivative of $y$ with respect to $x_{i_1}^{(1)}$ is \n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_{i_1}^{(1)}} = \\sum_{i_0 = 0}^{n_0-1} \\frac{\\partial x_{i_0}^{(0)}}{\\partial x_{i_1}^{(1)}}\n",
    "\\cdot\n",
    "\\frac{\\partial y}{\\partial x_{i_0}^{(0)}}.\n",
    "$$\n",
    "If \n",
    "$$\n",
    "x_{i_1}^{(1)} = f_{i_1}^{(1)}(x_0^{(2)}, x_1^{(2)}, \\ldots, x_{n_2-1}^{(2)})\n",
    "$$\n",
    "for all $i_1 = 0, 1, \\ldots, n_1-1$, then substituting\n",
    "$$\n",
    "x_{i_0}^{(0)} = f_{i_0}^{(0)}(x_0^{(1)}, x_1^{(1)}, \\ldots, x_{n_1-1}^{(1)})\n",
    "$$\n",
    "into \n",
    "$$\n",
    "y = f(x_0^{(0)}, x_1^{(0)}, \\ldots, x_{n_0-1}^{(0)}),\n",
    "$$\n",
    "for all $i_0 = 0,1,\\ldots,n_0-1$, gives $y$ in terms of $x_{i_1}^{(1)}$ for all $i_1 = 0,1,\\ldots,n_1-1$;\n",
    "therefore, by the multivariable chain rule the partial derivative of $y$ with respect to $x_{i_2}^{(2)}$ is \n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_{i_2}^{(2)}} = \\sum_{i_1 = 0}^{n_1-1} \\frac{\\partial x_{i_1}^{(1)}}{\\partial x_{i_2}^{(2)}}\n",
    "\\cdot\n",
    "\\frac{\\partial y}{\\partial x_{i_1}^{(1)}}.\n",
    "$$\n",
    "Repeat $N-3$ times to get \n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_{i_{N-1}}^{(N-1)}} = \\sum_{i_{N-2} = 0}^{n_{N-2}-1} \\frac{\\partial x_{i_{N-2}}^{(N-2)}}{\\partial x_{i_{N-1}}^{(N-1)}}\n",
    "\\cdot\n",
    "\\frac{\\partial y}{\\partial x_{i_{N-2}}^{(N-2)}}.\n",
    "$$\n",
    "\n",
    "*Neural Networks*\n",
    "\n",
    "To train a neural network using the gradient descent algorithm it is necessary to find the partial derivatives of such a composite function with respect to $x_i^{(j)}$ for all relevant $i$ and $j$.\n",
    "This is achieved through an algorithm called backpropagation that makes use of such a repetition of the multivariable chain rule.\n",
    "\n",
    "For a given point, where the value of $\\frac{\\partial y}{\\partial x_{i_0}^{(0)}}$ is known for all $i_0 = 0,1,\\ldots,n_0-1$ and the value of $\\frac{\\partial x_{i_{j}}^{(j)}}{\\partial x_{i_{j+1}}^{(j+1)}}$ is known for all relevant $j$, the value of $\\frac{\\partial y}{\\partial x_{i_1}^{(1)}}$ is calculable by substituting the known values of $\\frac{\\partial y}{\\partial x_{i_0}^{(0)}}$ and $\\frac{\\partial x_{i_0}^{(0)}}{\\partial x_{i_1}^{(1)}}$ into the equation\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_{i_1}^{(1)}} = \\sum_{i_0 = 0}^{n_0-1} \\frac{\\partial x_{i_0}^{(0)}}{\\partial x_{i_1}^{(1)}}\n",
    "\\cdot\n",
    "\\frac{\\partial y}{\\partial x_{i_0}^{(0)}}\n",
    "$$\n",
    "for all $i_1 = 0,1,\\ldots,n_1-1$.  \n",
    "This process is repeated until the value of $\\frac{\\partial y}{\\partial x_{i_{N-1}}^{(N-1)}}$ is calculated for all\n",
    "$i_{N-1} = 0,1,\\ldots,n_{N-1}-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Feedforward Artificial Neural Networks\n",
    "\n",
    "**Contents of Section 1**\n",
    "\n",
    "- Section 1.1 Activation Functons\n",
    "- Section 1.2 Artificial Neurons\n",
    " - Section 1.2.1 Bias\n",
    " - Section 1.2.2 Notation\n",
    " - Section 1.2.3 Graphical Representation\n",
    " - Section 1.2.4 Vector Representation\n",
    " - Section 1.2.5 Example\n",
    " - Section 1.2.6 Computing a Layer of Activations in Python\n",
    "- Section 1.3 Structure of a Feedforward Artificial Neural Network\n",
    " - Section 1.3.1 Initialisation\n",
    " - Section 1.3.2 Feedforward Pass\n",
    " - Section 1.3.3 Representation of a Feedforward Artificial Neural Network in Python\n",
    " - Section 1.3.4 Example 2\n",
    "- Section 1.4 Cost Function\n",
    "- Section 1.5 Gradient Descent\n",
    " - Section 1.5.1 The Theory Behind the Gradient Descent Algorithm\n",
    " - Section 1.5.2 Alternative Versions of Gradient Descent\n",
    "   - Section 1.5.2.1 Stochastic Gradient Descent\n",
    "   - Section 1.5.2.2 Min-Batch Gradient Descent\n",
    "- Section 1.6 Backward Propagation\n",
    " - Section 1.6.1 Notation\n",
    " - Section 1.6.2 The Backward Propagation Algorithm\n",
    "   - Section 1.6.2.1 Step 1. Calculating the Partial Derivative of $C(W, B, X, y)$ with respect to the Weights\n",
    "   - Section 1.6.2.2 Step 2. Calculating the Partial Derivative of $C(W, B, X, y)$ with respect to the Biases\n",
    "   - Section 1.6.2.3 Step 3. Calculating the Partial Derivative of $C(W, B, X, y)$ with respect to the Activations\n",
    " - Section 1.6.3 Notation\n",
    " - Section 1.6.4 The Derivative of the Activation Function\n",
    " - Section 1.6.5 Calculating the Partial Derivative of $C(W, B)$ for all Weights and Biases\n",
    "- Section 1.7 Representation of Gradient Descent with Backward Propagation in Python\n",
    "- Section 1.8 Worked Example\n",
    "\n",
    "**Overview of Section 1**\n",
    "\n",
    "Section 1 starts by introducing the concept of activation functions that are used to calculate the output of artificial neurons that are the key components of an artificial neural network (ANN) which will be introduced and fully explained in Section 1.2 and Section 1.3 respectively.\n",
    "Only supervised learning is considered here, so the ANN receives its input in the form of an input vector $X$ and its output will be compared to the target vector $y$ using the cost function as detailed in Section 1.4.\n",
    "Training the network requires minimising the cost function which is done here with the gradient descent algorithm using the backward propagation algorithm to calculate the gradients as detailed in Section 1.5 and Section 1.6 respectively.\n",
    "These ideas are combined in code in Section 1.7.\n",
    "The Sciket-Learn digits example is worked through in Section 1.8, which includes preprocessing, training the ANN created in Sections 1.3.3 and 1.7 with the three principle gradient descent methods and testing each.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 Activation Functions\n",
    "\n",
    "An *Activation Function* is a function the output of which changes state or crosses some threshold when its input changes state or crosses some threshold.   \n",
    "In Artificial Neural Networks the most common activation functions are the identity function, the Rectified Linear Unit (rectifier or ReLU), the sigmoid function and the hyperbolic tangent function.  \n",
    "\n",
    "The identity function is defined by \n",
    "$$\n",
    "\\rm{I}(x) = x\n",
    "$$\n",
    "and its output changes from a negative state for $x < 0$ to a positive state for $x > 0$.  \n",
    "The rectified linear unit is defined by \n",
    "$$\n",
    "\\rm{ReLU}(x) = \\rm{max}(0,x)\n",
    "$$\n",
    "and its output changes from a positive state for $x > 0$ to $0$ for $x \\leq 0$.  \n",
    "The sigmoid function is defined by\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "and its output ranges from $0$ to $1$ and changes state from the state $\\sigma(x) < \\frac{1}{2}$ for $x < 0$ to $\\sigma(x) > \\frac{1}{2}$ for $x > 0$.  \n",
    "The hyperbolic tangent function is defined by\n",
    "$$\n",
    "\\rm{tanh}(x) = 2 \\sigma(2x) - 1\n",
    "$$\n",
    "and its output ranges from $-1$ to $1$ and changes state from the state $\\rm{tanh}(x) < 0$ for $x < 0$ to $\\rm{tanh}(x) > 0$ for $x > 0$.  \n",
    "\n",
    "These functions are written as follows in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "y1 = ReLU(x)\n",
    "y2 = sigmoid(x)\n",
    "y3 = tanh(x)\n",
    "\n",
    "plt.plot(x,y1, label=\"ReLU\")\n",
    "plt.plot(x,y2, label=\"sigmoid\")\n",
    "plt.plot(x,y3, label=\"tanh\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Artificial Neurons\n",
    "\n",
    "An *Artificial Neuron, Node or Unit* is a function that takes a set of inputs, applies an activation function to their weighted sum and returns a single value, which is called the node's activation.\n",
    "If the activation is confined to $0$ and $1$ as would be the case with a binary step activation function, then the node is called a *Perceptron*, but for later purposes, namely Gradient Descent, only continuous activations are considered.\n",
    "\n",
    "An *Artificial Neural Network* is a collection of nodes organised into a sequence of layers with connections representing the flow of information between nodes.\n",
    "The type of artificial neural network under consideration here is the *Fully Connected Feedforward Neural Network*, such a network is characterised by the fact that information flows through the network in one direction only; that is, from the $L$-th layer to the $(L+1)$-th layer for all $L$.\n",
    "Fully connected indicates that the set of inputs received by a node in the $(L+1)$-th layer of the network is the complete set of  activations of the nodes in the $L$-th layer and the output of a further node, the $L$-th *Bias Node*, that is always the value $+1$; therefore, if the $L$-th layer possesses $n$ nodes, then every node in the $(L+1)$-th layer receives a set of $n+1$ inputs consisting of the $n$ activations returned by the $L$-th layer, one for each node, and the value $+1$.  \n",
    "\n",
    "The weight assigned to the output of the $k$-th node in the $L$-th layer by the $j$-the node in the $(L+1)$-th is denoted by $w_{j k}^{(L)}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2.1 Bias\n",
    "\n",
    "The weight assigned to the input value $1$ is called *The Bias* and the bias of the $j$-th node in the $(L+1)$-th layer is denoted by $b_j^{(L)}$.\n",
    "The hyperplane \n",
    "$$\n",
    "\\sum_{\\forall k} x_k w_{j k}^{(L)} = 0\n",
    "$$\n",
    "passes through the origin; therefore, the bias is added to remove this restriction and allow hyperplanes that do not pass through the origin.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2.2 Notation\n",
    "\n",
    "The output of the $j$-th node of the $i$-th layer is denoted by $a_{j}^{(L)}$.\n",
    "\n",
    "The weighted sum of the set of inputs of the $j$-th node of the $(L+1)$-th layer is denoted by\n",
    "$$\n",
    "z_j^{(L+1)} = b_j^{(L)} + \\sum_{k=0}^{n_L - 1} w_{j k}^{(L)} a_k^{(L)},\n",
    "$$\n",
    "so its output is $a_j^{(L+1)} = f(z_j^{(L+1)})$, where $f$ is the activation function and $n_L$ is the number of nodes in the $L$-th layer.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2.3 Graphical Representation\n",
    "\n",
    "A node of this type is represented graphically in Image 1.\n",
    "\n",
    "### Image 1\n",
    "\n",
    "<img src=\"NNEx1fNodeL.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2.4 Vector Representation\n",
    "\n",
    "Let $A^{(L)}$ be the column vector the elements of which are the activations of the $L$-th layer and let $W_j^{(L)}$ be the row vector the elements of which are the weights assigned to the activations of the $L$-th layer by the $j$-th node in the $(L+1)$-th layer, then \n",
    "$$\n",
    "a_j^{(L+1)} = f(z_j^{(L+1)})\n",
    "=\n",
    "f\\big(b_j^{(L)} + \\sum_{k = 0}^{n_L - 1} a_k^{(L)} w_{j k}^{(L)}\\big)\n",
    "=\n",
    "f\\big(b_j^{(L)} +\n",
    "\\begin{pmatrix}\n",
    "w_{j, 0}^{(L)} &\n",
    "w_{j, 1}^{(L)} &\n",
    "w_{j, 2}^{(L)} &\n",
    "\\dots &\n",
    "w_{j, n_L - 1}^{(L)}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a_0^{(L)} \\\\\n",
    "a_1^{(L)} \\\\\n",
    "a_2^{(L)} \\\\\n",
    "\\vdots \\\\\n",
    "a_{n_L - 1}^{(L)}\n",
    "\\end{pmatrix}\\big)\n",
    "=  f(b_j^{(L)} + W_j^{(L)} A^{(L)}).\n",
    "$$\n",
    "\n",
    "Further, let $W^{(L)}$ be the matrix with rows $W_0^{(L)}, W_1^{(L)}, W_2^{(L)}, \\ldots, W_{n_{L+1}-1}^{(L)}$, then $A^{(L+1)}$ is calculated as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "A^{(L+1)}\n",
    "&=\n",
    "\\begin{pmatrix}\n",
    "f(b_0^{(L)} + \\sum_{k = 0} a_k^{(L)} w_{0, k}^{(L)}) \\\\\n",
    "f(b_1^{(L)} + \\sum_{k = 0} a_k^{(L)} w_{1, k}^{(L)}) \\\\\n",
    "f(b_2^{(L)} + \\sum_{k = 0} a_k^{(L)} w_{2, k}^{(L)}) \\\\\n",
    "\\vdots \\\\\n",
    "f(b_{n_{L+1}-1}^{(L)} + \\sum_{k = 0} a_k^{(L)} w_{n_{L+1}-1, k}^{(L)})\n",
    "\\end{pmatrix} \\\\\n",
    "&= \n",
    "f\\Bigg(\n",
    "\\begin{pmatrix}\n",
    "b_0^{(L)} \\\\\n",
    "b_1^{(L)} \\\\\n",
    "b_2^{(L)} \\\\\n",
    "\\vdots \\\\\n",
    "b_{n_{L+1}-1}^{(L)}\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "w_{0, 0}^{(L)} & w_{0, 1}^{(L)} & w_{0, 2}^{(L)} \\dots w_{0, n_L - 1}^{(L)} \\\\\n",
    "w_{1, 0}^{(L)} & w_{1, 1}^{(L)} & w_{1, 2}^{(L)} \\dots w_{1, n_L - 1}^{(L)} \\\\\n",
    "w_{2, 0}^{(L)} & w_{2, 1}^{(L)} & w_{2, 2}^{(L)} \\dots w_{2, n_L - 1}^{(L)} \\\\\n",
    "\\vdots \\\\\n",
    "w_{n_{L+1}-1, 0}^{(L)} & w_{n_{L+1}-1, 1}^{(L)} & w_{n_{L+1}-1, 2}^{(L)} \\dots w_{n_{L+1}-1, n_L - 1}^{(L)}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a_0^{(L)} \\\\\n",
    "a_1^{(L)} \\\\\n",
    "a_2^{(L)} \\\\\n",
    "\\vdots \\\\\n",
    "a_{n_L - 1}^{(L)}\n",
    "\\end{pmatrix}\n",
    "\\Bigg) \\\\\n",
    "&= f(B^{(L)} + W^{(L)} A^{(L)}) = f(Z^{(L+1)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $B^{(L)} = (b_0^{(L)}, b_1^{(L)}, b_2^{(L)}, \\ldots b_{n_{L+1}-1}^{(L)})^T$\n",
    "and $Z^{(L+1)} = (z_0^{(L+1)}, z_1^{(L+1)}, z_2^{(L+1)}, \\ldots z_{n_{L+1}-1}^{(L+1)})^T$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2.5 Example\n",
    "\n",
    "Let the $L$-th layer of a network possess $5$ nodes and let those $5$ nodes return the values $a_0^{(L)} = 0.501$, $a_1^{(L)} = 0.248$, $a_2^{(L)} = 0.115$, $a_0^{(L)} = 0.288$ and $a_0^{(L)} = 0.657$, respectively.\n",
    "Therefore, the $j$-th node in the $(L+1)$-th layer receives the set of inputs $\\{0.501, 0.248, 0.115, 0.288, 0.657, 1\\}$ and applies the activation function (assume that ReLU is used) to their weighted sum\n",
    "$$\n",
    "z_j^{(L+1)} = 0.501 w_{j 0}^{(L)} + 0.248 w_{j 1}^{(L)} + 0.115 w_{j 1}^{(L)} + 0.288 w_{j 1}^{(L)} + 0.657 w_{j 1}^{(L)} + b_j^{(L)}\n",
    "$$\n",
    "and returns the result; that is, it returns\n",
    "$$\n",
    "\\rm{ReLU}(z_j^{(L+1)}) = \n",
    "\\rm{max}(0, 0.501 w_{j 0}^{(L)} + 0.248 w_{j 1}^{(L)} + 0.115 w_{j 1}^{(L)} + 0.288 w_{j 1}^{(L)} + 0.657 w_{j 1}^{(L)} + b_j^{(L)}).\n",
    "$$\n",
    "This node is represented graphically in Image 2.\n",
    "\n",
    "### Image 2\n",
    "\n",
    "<img src=\"NNEx1ReLUNodeL.png\">\n",
    "\n",
    "The calculation of the activation is written in vector form as follows:\n",
    "\n",
    "$$\n",
    "a_j^{(L+1)} = \\rm{ReLU} (z_j^{(L+1)}) =  \\rm{ReLU} (b_j^{(L)} + W_j^{(L)} A^{(L)})\n",
    "=\n",
    "\\rm{ReLU} \\big(b_j^{(L)} +\n",
    "\\begin{pmatrix}\n",
    "w_{j 0}^{(L)} &\n",
    "w_{j 1}^{(L)} &\n",
    "w_{j 2}^{(L)} &\n",
    "w_{j 3}^{(L)} &\n",
    "w_{j 4}^{(L)}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0.501 \\\\\n",
    "0.248 \\\\\n",
    "0.115 \\\\\n",
    "0.288 \\\\\n",
    "0.657\n",
    "\\end{pmatrix}\n",
    "\\big)\n",
    "=\n",
    "\\rm{ReLU} \\big(b_j^{(L)} + 0.501 w_{j 0}^{(L)} + 0.248 w_{j 1}^{(L)} + 0.115 w_{j 1}^{(L)} + 0.288 w_{j 1}^{(L)} + 0.657 w_{j 1}^{(L)}\\big).\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2.6 Computing a Layer of Activations in Python\n",
    "\n",
    "Extend Example 1 to generate the activations of every node in the $(L+1)$-th layer.\n",
    "\n",
    "First assume that the $(L+1)$-th layer possesses $8$ nodes.\n",
    "As $A^{(L)}$ will be represented as a row vector in NumPy, that is, a NumPy array of shape $(1,n_L)$ the transposed equation\n",
    "$$\n",
    "(A^{(L+1)})^T = f((Z^{(L+1)})^T) = f((B^{(L)})^T + (A^{(L)})^T (W^{(L)})^T)\n",
    "$$\n",
    "is used in place of \n",
    "$$\n",
    "A^{(L+1)} = f(Z^{(L+1)}) = f(B^{(L)} + W^{(L)} A^{(L)}).\n",
    "$$\n",
    "Perform the calculation to determine the activations of the $(L+1)$-th layer in Python as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_L.shape =  (5, 8)\n",
      "\n",
      "B_L.shape =  (1, 8)\n",
      "\n",
      "A_L.shape =  (1, 5)\n",
      "\n",
      "Z_L_1 = \n",
      "[[ 0.75318796  0.73385401 -0.93759982 -0.27173855  0.98125529 -1.18561565\n",
      "  -0.0506199  -1.5007594 ]]\n",
      "Z_L_1.shape =  (1, 8)\n",
      "\n",
      "A_L_1 = \n",
      "[[0.75318796 0.73385401 0.         0.         0.98125529 0.\n",
      "  0.         0.        ]]\n",
      "A_L_1.shape =  (1, 8)\n"
     ]
    }
   ],
   "source": [
    "W_L = 2*np.random.random((5,8))-1 # (W^{(L)})^T\n",
    "print(\"W_L.shape = \", W_L.shape)\n",
    "print()\n",
    "\n",
    "B_L = 2*np.random.random((1,8))-1 # (B^{(L)})^T\n",
    "print(\"B_L.shape = \", B_L.shape)\n",
    "print()\n",
    "\n",
    "A_L = np.array([[0.501, 0.248, 0.115, 0.288, 0.657]]) # (A^{(L)})^T\n",
    "print(\"A_L.shape = \", A_L.shape)\n",
    "print()\n",
    "\n",
    "Z_L_1 = B_L + np.matmul(A_L, W_L) # (Z^{(L+1)})^T = (B^{(L)})^T + (A^{(L)})^T (W^{(L)})^T\n",
    "print(\"Z_L_1 = \")\n",
    "print(Z_L_1)\n",
    "print(\"Z_L_1.shape = \", Z_L_1.shape)\n",
    "print()\n",
    "\n",
    "A_L_1 = ReLU(Z_L_1) # (A^{(L+1)})^T = f((Z^{(L+1)})^T)\n",
    "print(\"A_L_1 = \")\n",
    "print(A_L_1)\n",
    "print(\"A_L_1.shape = \", A_L_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Structure of a Feedforward Artificial Neural Network\n",
    "\n",
    "As only supervised learning is considered here, label the input vector $X$ and the corresponding target vector $y$.\n",
    "If the networks input vector is $X = (x_0, x_1, \\ldots, x_{n_0 - 1})$, then the sole input of the $k$-th node in the $0$-th layer or *Input Layer* is the value $x_k$ and there are exactly $n_0$ nodes in the input layer, one for each feature of the input vector.\n",
    "If $y = ( y_0, x_1, \\ldots, y_{n_{N-1}-1} )$ is the corresponding target vector, then the output of the $j$-th node in the \n",
    "$(N-1)$-th or *Output Layer* is the value $\\bar{y}_j$, which is to be compared with $y_j$, and there are exactly $n_{N-1}$ nodes in the output layer, one for each feature of the target vector.\n",
    "Every layer between the input layer and the output layer is called a *Hidden Layer*.  \n",
    "\n",
    "When combined with the description of nodes and layers in Section 1.2 it follows that a feedforward neural network is a *Directed Acyclic Graph*, where a directed edge (or connection) from the $k$-th node in the $L$-th layer to the $j$-th node in the $(L+1)$-th layer indicates that $a_k^{(L)}$ is in the input set of the $j$-th node in the $(L+1)$-th layer; this edge is typically labelled with the weight $w_{j k}^{(L)}$.\n",
    "In such a graph the nodes without parents (excluding the bias nodes) are the input nodes and the nodes without children are output nodes.\n",
    "\n",
    "A feedforward neural network with $4$ layers, an input layer with $3$ nodes, a hidden layer with $4$ nodes, a hidden layer with $2$ nodes and an output layer with $2$ nodes is depicted in Image 3.\n",
    "\n",
    "### Image 3\n",
    "\n",
    "<img src=\"FFNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Initialisation\n",
    "\n",
    "Unless there exist saved weights and biases from previous training, all weights and biases are initialised randomly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.3.2 Feedforward Pass\n",
    "\n",
    "The process of passing a given feature set through the network and calculating an output is called a *Feedforward Pass*.\n",
    "The simplicity of a feedforward pass is demonstrated in the following algorithm.\n",
    "\n",
    "## Algorithm: Feedforward Pass\n",
    "\n",
    "${\\bf{\\text{Input}}}: \\ X$\n",
    "\n",
    "$A^{(0)} := X$\n",
    "\n",
    "${\\bf{\\text{for }}} L = 0, 1, 2, \\ldots, N-3, N-2, {\\bf{\\text{do:}}}$\n",
    "\n",
    "1. Use $A^{(L)}$, $B^{(L)}$ and $W^{(L)}$ to calculate $Z^{(L+1)}$ with the equation\n",
    "$$\n",
    "Z^{(L+1)} = B^{(L)} + W^{(L)} A^{(L)}.\n",
    "$$\n",
    "\n",
    "2. Use $Z^{(L+1)}$ to calculate $A^{(L+1)}$ with the equation\n",
    "$$\n",
    "A^{(L+1)} = f(Z^{(L+1)}),\n",
    "$$\n",
    "where $f$ is the activation function.\n",
    "\n",
    "${\\bf{\\text{Output}}}: \\ A^{(N-1)}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.3.3 Representation of a Feedforward Artificial Neural Network in Python\n",
    "\n",
    "- Class *connections* - Represents the weights and biases; in a feedforward pass they are treated like constants in an equation.\n",
    "- Class *network* - Represents the artificial neural network.\n",
    " - Method *add_layer(n)* - Adds a new layer of *n* artificial neurons to the network, by adding a new layer of weights and biases.\n",
    " - Method *forward_pass(X)* - Performs a feedforward pass with the input vector (or the input vectors) $X$ and returns $Z$ and $A$, that is the set of all appropriate $(Z^{(L)})^T$ and $(A^{(L)})^T$ for $L = 0, 1, \\ldots, N-1$; obviously $(Z^{(0)})^T = \\emptyset$.\n",
    " Note that for $s$ input vectors all row vectors $(A^{(L)})^T$ are in a Numpy array of shape $(s,n_{L})$ in $A$; similarly for $(Z^{(L)})^T$.\n",
    "\n",
    "**Note:** The method *forward_pass* is designed to accept an input vector as a NumPy array of shape $(1,n_0)$, for example\n",
    "\n",
    "$$\n",
    "X = np.array([[0.298, -0.609, 0.987]])),\n",
    "$$\n",
    "\n",
    "where $n_0 = 3$ and $s>1$ input vectors as a NumPy array of shape $(s, n_0)$, for example\n",
    "\n",
    "$$\n",
    "S = np.array([[0.298, -0.609, 0.987], [0.390, 0.492, -0.127]])),\n",
    "$$\n",
    "\n",
    "where $s = 2$ and $n_0 = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class connections(object):\n",
    "    def __init__(self, m, n):\n",
    "        self._W = 2*np.random.random((m,n))-1 # Weights\n",
    "        self._B = 2*np.random.random((1,n))-1 # Biases\n",
    "        \n",
    "    @property\n",
    "    def W(self):\n",
    "        return self._W\n",
    "    \n",
    "    @W.setter\n",
    "    def W(self, W):\n",
    "        self._W = W\n",
    "    \n",
    "    @property\n",
    "    def B(self):\n",
    "        return self._B\n",
    "    \n",
    "    @B.setter\n",
    "    def B(self, B):\n",
    "        self._B = B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    def __init__(self):\n",
    "        self._nodes_per_layer = []\n",
    "        self._all_connections = []\n",
    "    \n",
    "    def add_layer(self, n):\n",
    "        if len(self._nodes_per_layer) == 0: # Empty network\n",
    "            # Input layer\n",
    "            self._nodes_per_layer.append(n)\n",
    "        else:\n",
    "            # Subsequent layers\n",
    "            WB = connections(self._nodes_per_layer[-1], n)\n",
    "            self._nodes_per_layer.append(n)\n",
    "            self._all_connections.append(WB)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        if len(self._nodes_per_layer) != 0: # Not an empty network\n",
    "            Z = [] # To contain Z^{(0)}^T, Z^{(1)}^T, ..., Z^{(N-1)}^T\n",
    "            A = [] # To contain Z^{(0)}^T, Z^{(1)}^T, ..., Z^{(N-1)}^T\n",
    "            Z_L = np.array([]) # Z^{(L)}^T\n",
    "            A_L = np.copy(X)   # A^{(L)}^T\n",
    "            Z.append(Z_L)\n",
    "            A.append(A_L)\n",
    "            A_L_1 = np.array([]) # A^{(L+1)}^T\n",
    "            for connections_L in self._all_connections:\n",
    "                Z_L_1 = connections_L.B + np.matmul(A_L, connections_L.W) # Z^{(L+1)}^T = (B^{(L)})^T + (A^{(L)})^T (W^{(L)})^T\n",
    "                Z.append(Z_L_1)\n",
    "                A_L_1 = sigmoid(Z_L_1) # a^{(L+1)}^T = f(Z^{(L+1)}^T)\n",
    "                A.append(A_L_1)\n",
    "                A_L = np.copy(A_L_1)\n",
    "            return Z, A\n",
    "        else:\n",
    "            return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Example 2\n",
    "\n",
    "Create *empty_network* an instance of the class *network* without layers and create *example_network* an instance of the class *network* with the layers depicted in image 3.  \n",
    "\n",
    "Pass the input vector\n",
    "$$\n",
    "X = ( 0.298, -0.609, 0.987 )\n",
    "$$\n",
    "through the networks.  \n",
    "\n",
    "Pass the set of $5$ input vectors\n",
    "$$\n",
    "\\begin{align}\n",
    "S = \\{ &( 0.298, -0.609, 0.987 ), \\\\\n",
    "&( 0.390, 0.492, -0.127 ), \\\\\n",
    "&( 1.00, -0.60, -0.005 ), \\\\\n",
    "&( 0.0, 0.503, 0.114 ), \\\\\n",
    "&( -0.718, -0.609, -0.25 ) \\}\n",
    "\\end{align}\n",
    "$$\n",
    "through the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Z[ 0 ] =\n",
      "[]\n",
      "Z[ 1 ] =\n",
      "[[0.13231424 0.60489032 0.14677716 0.96238425]]\n",
      "Z[ 2 ] =\n",
      "[[0.69983819 1.00124792]]\n",
      "Z[ 3 ] =\n",
      "[[0.715593   1.73752837]]\n",
      "\n",
      "A[ 0 ] =\n",
      "[[ 0.298 -0.609  0.987]]\n",
      "A[ 1 ] =\n",
      "[[0.53303039 0.64677434 0.53662855 0.72359892]]\n",
      "A[ 2 ] =\n",
      "[[0.6681519  0.73130386]]\n",
      "A[ 3 ] =\n",
      "[[0.67163583 0.85037285]]\n",
      "\n",
      "Z[ 0 ] =\n",
      "[]\n",
      "Z[ 1 ] =\n",
      "[[ 0.13231424  0.60489032  0.14677716  0.96238425]\n",
      " [ 0.16535526 -0.33803144 -1.07463633  0.16127563]\n",
      " [ 0.1806452  -0.3906823  -0.93411668  0.52630241]\n",
      " [ 0.04322004  0.04097647 -0.737872    0.15985824]\n",
      " [-0.79802045  0.57634176 -0.53293163 -0.51017823]]\n",
      "Z[ 2 ] =\n",
      "[[0.69983819 1.00124792]\n",
      " [0.64047627 1.04947799]\n",
      " [0.58186127 1.07951273]\n",
      " [0.71013571 0.99541388]\n",
      " [0.95567226 0.70619311]]\n",
      "Z[ 3 ] =\n",
      "[[0.715593   1.73752837]\n",
      " [0.71738211 1.74472234]\n",
      " [0.71883281 1.74838486]\n",
      " [0.71532946 1.7367383 ]\n",
      " [0.70608524 1.68543963]]\n",
      "\n",
      "A[ 0 ] =\n",
      "[[ 0.298 -0.609  0.987]\n",
      " [ 0.39   0.492 -0.127]\n",
      " [ 1.    -0.6   -0.005]\n",
      " [ 0.     0.503  0.114]\n",
      " [-0.718 -0.609 -0.25 ]]\n",
      "A[ 1 ] =\n",
      "[[0.53303039 0.64677434 0.53662855 0.72359892]\n",
      " [0.54124488 0.41628774 0.25452238 0.54023174]\n",
      " [0.54503889 0.40355306 0.28209028 0.6286203 ]\n",
      " [0.51080333 0.51024269 0.32346965 0.53987967]\n",
      " [0.31044912 0.64022521 0.36983339 0.37515175]]\n",
      "A[ 2 ] =\n",
      "[[0.6681519  0.73130386]\n",
      " [0.65486111 0.74067465]\n",
      " [0.64149557 0.74640176]\n",
      " [0.67043115 0.73015594]\n",
      " [0.72225448 0.66955943]]\n",
      "A[ 3 ] =\n",
      "[[0.67163583 0.85037285]\n",
      " [0.67203028 0.8512859 ]\n",
      " [0.67234994 0.85174897]\n",
      " [0.6715777  0.85027229]\n",
      " [0.66953556 0.84362348]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empty_network = network()\n",
    "\n",
    "example_network = network()\n",
    "\n",
    "example_network.add_layer(3)\n",
    "example_network.add_layer(4)\n",
    "example_network.add_layer(2)\n",
    "example_network.add_layer(2)\n",
    "\n",
    "X = np.array([[0.298, -0.609, 0.987]])\n",
    "\n",
    "Z, A = empty_network.forward_pass(X)\n",
    "\n",
    "for i in range(len(Z)):\n",
    "    print(\"Z[\",i,\"]\")\n",
    "    print(Z[i])\n",
    "print()\n",
    "\n",
    "for i in range(len(A)):\n",
    "    print(\"A[\",i,\"] =\")\n",
    "    print(A[i])\n",
    "print()\n",
    "\n",
    "Z, A = example_network.forward_pass(X)\n",
    "\n",
    "for i in range(len(Z)):\n",
    "    print(\"Z[\",i,\"] =\")\n",
    "    print(Z[i])\n",
    "print()\n",
    "\n",
    "for i in range(len(A)):\n",
    "    print(\"A[\",i,\"] =\")\n",
    "    print(A[i])\n",
    "print()\n",
    "\n",
    "S = np.array([[0.298, -0.609, 0.987], [0.390, 0.492, -0.127], \n",
    "              [1.00, -0.60, -0.005], [0.0, 0.503, 0.114], [-0.718, -0.609, -0.25]])\n",
    "\n",
    "Z, A = example_network.forward_pass(S)\n",
    "\n",
    "for i in range(len(Z)):\n",
    "    print(\"Z[\",i,\"] =\")\n",
    "    print(Z[i])\n",
    "print()\n",
    "\n",
    "for i in range(len(A)):\n",
    "    print(\"A[\",i,\"] =\")\n",
    "    print(A[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Cost Function\n",
    "\n",
    "For a given input vector $X = ( x_0, x_1, \\ldots, x_{n_0 - 1} )$, assume that the network returns the vector $\\bar{y} = ( \\bar{y}_0, \\bar{y}_1, \\ldots, \\bar{y}_{n_{N-1}-1} )$, then the *Cost Function* of the training sample $(X, y)$ is given by\n",
    "$$\n",
    "\\begin{align}\n",
    "C(W, B, X, y) &= \\frac{1}{2} || y - \\bar{y} ||_2^2 \\\\\n",
    "&= \\frac{1}{2} \\sum_{i = 0}^{n_{N-1}-1}  | y_i - \\bar{y}_i |^2,\n",
    "\\end{align}\n",
    "$$\n",
    "which is $\\frac{1}{2}$ of what is called the sum-of-square error (SSE), where $|| \\cdot ||_2$ is called the $L^2$-norm or the Euclidean Norm.  \n",
    "\n",
    "Since the cost function $C(W, B, X, y)$ gives $\\frac{1}{2}$ the sum-of-square error for only a single training sample, the *Cost Function* for the whole training set $S$ of $m$ training examples is the average of $C(W, B, X, y)$ over all $(X, y) \\in S$; that is,\n",
    "$$\n",
    "\\begin{align}\n",
    "C(W, B) &= \\frac{1}{m} \\sum_{\\forall (X,y) \\in S} C(W, B, X, y) \\\\\n",
    "&= \\frac{1}{2m} \\sum_{\\forall (X,y) \\in S}  || y - \\bar{y} ||_2^2 \\\\\n",
    "&= \\frac{1}{2m} \\sum_{\\forall (X,y) \\in S} \\sum_{i = 0}^{n_{N-1}-1} | y_i - \\bar{y}_i |^2, \n",
    "\\end{align}\n",
    "$$\n",
    "which is $\\frac{1}{2}$ of the mean square error (MSE) of the $L^2$-norm over all training samples.\n",
    "\n",
    "**Note:** There are other possibilities for the cost function, with the only two restrictions being that \n",
    "$$\n",
    "C(W, B) = \\frac{1}{m} \\sum_{\\forall (X,y) \\in S} C(W, B, X, y)\n",
    "$$\n",
    "and $C(W,B)$ is not dependent on $A^{(L)}$ for $L < N-1$ beyond the dependence of $A^{(N-1)}$ on $A^{(N-2)}$, which is required for backward propagation.\n",
    "\n",
    "The aim of training a feedforward neural network is to minimise the cost function and one method of achieving this is with the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Gradient Descent\n",
    "\n",
    "*Gradient Descent* is the optimisation algorithm that will be used here to minimise the cost function $C(W, B)$.\n",
    "\n",
    "## Algorithm: Gradient Descent\n",
    "\n",
    "Let $\\bar{w}_{j k}^{(L)}$ and $\\bar{b}_j^{(L)}$ denote the current values of the $w_{j k}^{(L)}$ and $b_j^{(L)}$ for all $j$, $k$ and $L$.  \n",
    "\n",
    "> 1. Generate random values of $\\bar{w}_{j k}^{(L)}$ and $\\bar{b}_j^{(L)}$ for all $j$, $k$ and $L$.  \n",
    "\n",
    "${\\bf{\\text{for }}} epoch = 0, 1, \\ldots number\\_of\\_epochs-1 {\\bf{\\text{ do:}}}$\n",
    "\n",
    "> 2. Calculate \n",
    "$$\n",
    "\\frac{\\partial C(W, B)}{\\partial w_{j k}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}\n",
    "\\hspace{10pt} \\text{ and } \\hspace{10pt}\n",
    "\\frac{\\partial C(W, B)}{\\partial b_j^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}};\n",
    "$$\n",
    "that is, the partial derivative of $C(W, B)$ with respect to $w_{j k}^{(L)}$ at $(\\bar{W}, \\bar{B})$ and the partial derivative of $C(W, B)$ with respect to $w_{j k}^{(L)}$ at $(\\bar{W}, \\bar{B})$ for all $j$, $k$ and $L$.\n",
    "\n",
    "> 3. Update $\\bar{w}_{j k}^{(L)}$ to $\\bar{w}_{j k}^{(L)} - \\eta \\frac{\\partial C(W, B)}{\\partial w_{j k}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}$ for all $j$, $k$ and $L$.  \n",
    "Update $\\bar{b}_j^{(L)}$ to $\\bar{b}_j^{(L)} - \\eta \\frac{\\partial C(W, B)}{\\partial b_{j}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}$ for all $j$, $k$ and $L$, where the hyperparameter $\\eta$, called the *Learning Rate*, is set between $0$ and $1$.\n",
    "\n",
    "Each repetition of stages 2 and 3 is called an *Epoch*; similarly, for other gradient descent algorithm.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.5.1 The Theory Behind the Gradient Descent Algorithm\n",
    "\n",
    "A brief overview of the theory behind the gradient descent algorithm and its prerequisites is presented in Sections 0.2.1 to 0.2.5.\n",
    "The hope is that the values of $\\bar{W}$ and $\\bar{B}$ converge to a local minimum of the function $C(W, B)$, and the further hope is that the local minimum is a global minimum.\n",
    "\n",
    "In practice, if the gradient descent algorithm does not result in convergence, then reducing $\\eta$ or trying a new set of initial random values for the weights and biases may work.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.5.2 Alternative Versions of Gradient Descent\n",
    "\n",
    "The version of gradient descent described above is called *Batch Gradient Descent*; batch gradient descent is characterised by the fact that the updates are made using the cost function $C(W, B)$ and the set of all training samples $S$.\n",
    "*Stochastic Gradient Descent* and *Mini-Batch Gradient Descent* are versions of the gradient descent algorithm and both will be described here and demonstrated in example 3.\n",
    "Other gradient based algorithms are described in the very useful post: https://ruder.io/optimizing-gradient-descent/index.html#adagrad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.5.2.1 Stochastic Gradient Descent\n",
    "\n",
    "In *Stochastic Gradient Descent*, the updates are made for each individual training sample using $C(W, B, X, y)$ in place of $C(W, B)$ as follows.\n",
    "\n",
    "## Algorithm: Stochastic Gradient Descent\n",
    "\n",
    "Let $\\bar{w}_{j k}^{(L)}$ and $\\bar{b}_j^{(L)}$ denote the current values of the $w_{j k}^{(L)}$ and $b_j^{(L)}$ for all $j$, $k$ and $L$.\n",
    "\n",
    "> 1. Generate random values of $\\bar{w}_{j k}^{(L)}$ and $\\bar{b}_j^{(L)}$ for all $j$, $k$ and $L$.  \n",
    "\n",
    "${\\bf{\\text{for }}} epoch = 0, 1, \\ldots number\\_of\\_epochs-1 {\\bf{\\text{ do:}}}$\n",
    "\n",
    "> Shuffle all $(X,y)$ in $S$.\n",
    "\n",
    "> ${\\bf{\\text{for all }}} (X,y) \\in S \\text{(shuffled)} {\\bf{\\text{ do:}}}$\n",
    "\n",
    ">> 2. Calculate \n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial w_{j k}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}\n",
    "\\hspace{10pt} \\text{ and } \\hspace{10pt}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial b_j^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}};\n",
    "$$\n",
    "that is, the partial derivative of $C(W, B, X, y)$ with respect to $w_{j k}^{(L)}$ at $(\\bar{W}, \\bar{B})$ and the partial derivative of $C(W, B, X, y)$ with respect to $w_{j k}^{(L)}$ at $(\\bar{W}, \\bar{B})$ for all $j$, $k$ and $L$.\n",
    "\n",
    ">> 3. Update $\\bar{w}_{j k}^{(L)}$ to $\\bar{w}_{j k}^{(L)} - \\eta \\frac{\\partial C(W, B, X, y)}{\\partial w_{j k}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}$ for all $j$, $k$ and $L$.  \n",
    "Update $\\bar{b}_j^{(L)}$ to $\\bar{b}_j^{(L)} - \\eta \\frac{\\partial C(W, B, X, y)}{\\partial b_{j}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}$ for all $j$, $k$ and $L$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.5.2.2 Mini-Batch Gradient Descent\n",
    "\n",
    "In *Mini-Batch Gradient Descent*, the updates are made for subsets of $S$ of a given fixed size $n$, the *batch size*, using $\\frac{1}{n} \\sum_{\\forall (X,y) \\in S_i} C(W, B, X, y)$ in place of $C(W, B)$, where $S_i$ is a subset of $S$ of size $n$, as follows.\n",
    "\n",
    "## Algorithm: Mini-Batch Gradient Descent\n",
    "\n",
    "Let $\\bar{w}_{j k}^{(L)}$ and $\\bar{b}_j^{(L)}$ denote the current values of the $w_{j k}^{(L)}$ and $b_j^{(L)}$ for all $j$, $k$ and $L$.\n",
    "\n",
    "> 1. Generate random values of $\\bar{w}_{j k}^{(L)}$ and $\\bar{b}_j^{(L)}$ for all $j$, $k$ and $L$. \n",
    "\n",
    "${\\bf{\\text{for }}} epoch = 0, 1, \\ldots number\\_of\\_epochs-1 {\\bf{\\text{ do:}}}$\n",
    "\n",
    "> Shuffle all $(X,y)$ in $S$ to get a fresh ordering $S = \\{ (X_0,y_0), (X_1,y_1), \\ldots, (X_{n-1},y_{n-1}) \\}$.\n",
    "\n",
    "> ${\\bf{\\text{for }}} i = 0,1,2,\\ldots,\\lfloor \\frac{m}{n} \\rfloor {\\bf{\\text{ do:}}}$\n",
    "\n",
    ">> $S_i = \\{ (X_{mi}, y_{mi}), (X_{mi+1}, y_{mi+1}), \\ldots (X_{m(i+1)-1}, y_{m(i+1)-1}) \\} \\subset S$(shuffled).\n",
    "\n",
    ">> ${\\bf{\\text{for all }}} (X,y) \\in S_i {\\bf{\\text{ do:}}}$\n",
    "\n",
    ">>> 2. Calculate \n",
    "$$\n",
    "\\frac{1}{n} \\sum_{\\forall (X,y) \\in S_i} \\frac{\\partial C(W, B, X, y)}{\\partial w_{j k}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}\n",
    "\\hspace{10pt} \\text{ and } \\hspace{10pt}\n",
    "\\frac{1}{n} \\sum_{\\forall (X,y) \\in S_i} \\frac{\\partial C(W, B, X, y)}{\\partial b_j^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}};\n",
    "$$\n",
    "that is, the partial derivative of $\\frac{1}{n} \\sum_{\\forall (X,y) \\in S_i} C(W, B, X, y)$ with respect to $w_{j k}^{(L)}$ at $(\\bar{W}, \\bar{B})$ and the partial derivative of $\\frac{1}{n} \\sum_{\\forall (X,y) \\in S_i} C(W, B, X, y)$ with respect to $w_{j k}^{(L)}$ at $(\\bar{W}, \\bar{B})$ for all $j$, $k$ and $L$.\n",
    "\n",
    ">>> 3. Update $\\bar{w}_{j k}^{(L)}$ to $\\bar{w}_{j k}^{(L)} - \\frac{\\eta}{n} \\sum_{\\forall (X,y) \\in S_i} \\frac{\\partial C(W, B, X, y)}{\\partial w_{j k}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}$ for all $j$, $k$ and $L$.  \n",
    "Update $\\bar{b}_j^{(L)}$ to $\\bar{b}_j^{(L)} - \\frac{\\eta}{n} \\sum_{\\forall (X,y) \\in S_i} \\frac{\\partial C(W, B, X, y)}{\\partial b_{j}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}$ for all $j$, $k$ and $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Backward Propagation\n",
    "\n",
    "Backpropagation is the algorithm used to calculate $\\frac{\\partial C(W, B, X, y)}{\\partial w_{j k}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}$ and $\\frac{\\partial C(W, B, X, y)}{\\partial b_{j}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}$ for all $i, j$ and $k$.\n",
    "\n",
    "For a given training sample $(X, y)$ the cost function is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "C(W, B, X, y) &= \\frac{1}{2} \\sum_{i = 0}^{n_{N-1}-1}  | y_i - \\bar{y}_i |^2. \\\\\n",
    "&= \\frac{1}{2} \\sum_{i = 0}^{n_{N-1}-1}  | y_i - a_{i}^{(N-1)} |^2;\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "therefore, the partial derivative of $C(W, B, X, y)$ with respect to $a_i^{(N-1)}$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{i}^{(N-1)}} = a_{i}^{(N-1)} - y_i\n",
    "$$\n",
    "\n",
    "for all $i$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.6.1 Notation\n",
    "\n",
    "Let $A^{(L)}$, $Z^{(L)}$ and $W^{(L)}$ be as defined in Section 1.2.4 and define $\\frac{\\partial C(W, B, X, y)}{\\partial A^{(L)}}$ to be the column vector the $k$-th element of which is the partial derivative of $C(W, B, X, y)$ with respect to $a_k^{(L)}$ for all $k$; that is, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial A^{(L)}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{0}^{(L)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{1}^{(L)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{2}^{(L)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{n_L - 1}^{(L)}}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Similarly, define $\\frac{\\partial C(W, B, X, y)}{\\partial W^{(L)}}$ to be the matrix the element in the $j$-th row and $k$-th column of which is the partial derivative of $C(W, B, X, y)$ with respect to $w_{j k}^{(L)}$ for all $j$ and $k$ and define $\\frac{\\partial C(W, B, X, y)}{\\partial B^{(L)}}$ to be the column vector the $j$-th element of which is the partial derivative of $C(W, B, X, y)$ with respect to $b_j^{(L)}$ for all $j$; that is,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial W^{(L)}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial w_{0 0}^{(L)}} & \\frac{\\partial C(W, B, X, y)}{\\partial w_{0 1}^{(L)}} & \n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial w_{0 2}^{(L)}} & \\dots & \\frac{\\partial C(W, B, X, y)}{\\partial w_{0 n_L}^{(L)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial w_{1 0}^{(L)}} & \\frac{\\partial C(W, B, X, y)}{\\partial w_{1 1}^{(L)}} & \n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial w_{1 2}^{(L)}} & \\dots & \\frac{\\partial C(W, B, X, y)}{\\partial w_{1 n_i}^{(L)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial w_{2 0}^{(L)}} & \\frac{\\partial C(W, B, X, y)}{\\partial w_{2 1}^{(L)}} & \n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial w_{2 2}^{(L)}} & \\dots & \\frac{\\partial C(W, B, X, y)}{\\partial w_{2 n_L}^{(L)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial w_{n_{L+1} - 1 0}^{(L)}} & \\frac{\\partial C(W, B, X, y)}{\\partial w_{n_{L+1} - 1 1}^{(L)}} & \n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial w_{n_{L+1} - 1 2}^{(L)}} & \\dots & \\frac{\\partial C(W, B, X, y)}{\\partial w_{n_{L+1} - 1 n_L - 1}^{(L)}}\n",
    "\\end{pmatrix}\n",
    "\\hspace{5pt}\n",
    "\\text{and}\n",
    "\\hspace{5pt}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial B^{(L)}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial b_{0}^{(L)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial b_{1}^{(L)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial b_{2}^{(L)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial b_{n_{L+1} - 1}^{(L)}}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.6.2 The Backward Propagation Algorithm\n",
    "\n",
    "Since\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{j}^{(N-1)}} = a_{j}^{(N-1)} - y_j\n",
    "$$\n",
    "\n",
    "for all $j$, it follows that \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial A^{(N-1)}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{0}^{(N-1)} - y_0 \\\\\n",
    "a_{j}^{(N-1)} - y_j \\\\\n",
    "a_{j}^{(N-1)} - y_j \\\\\n",
    "\\vdots \\\\\n",
    "a_{n_{N-1}}^{(N-1)} - y_{n_{N-1}}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "## Algorithm: Backward Propagation\n",
    "\n",
    "The values of $\\frac{\\partial C(W, B, X, y)}{\\partial A^{(N-1)}}$ are known.  \n",
    "\n",
    "${\\bf{\\text{for }}} L = N-2, N-3, \\ldots, 2, 1, 0 {\\bf{\\text{ do:}}}$\n",
    "\n",
    "> 1. Use known values of $Z^{(L+1)}$, $\\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}}$ and $A^{(L)}$ to calculate the values of $\\frac{\\partial C(W, B, X, y)}{\\partial W^{(L)}}$ using the equation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial W^{(L)}} = \\big( f'(Z^{(L+1)}) \\circ \\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}} \\big) (A^{(L)})^T, \\tag{Equation 1}\n",
    "$$\n",
    "> where $f'$ is the derivative of $f$.\n",
    "\n",
    "> 2. Use known values of $Z^{(L)}$ and $\\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}}$ to calculate the values of $\\frac{\\partial C(W, B, X, y)}{\\partial B^{(L)}}$ using the equation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial B^{(L)}} = f'(Z^{(L+1)}) \\circ \\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}}.\n",
    "\\tag{Equation 2}\n",
    "$$\n",
    "\n",
    "> 3. Use known values of $W^{(L)}$, $Z^{(L+1)}$ and $\\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}}$ to calculate the values of $\\frac{\\partial C(W, B, X, y)}{\\partial A^{(L)}}$ using the equation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial A^{(L)}} = (W^{(L)})^T \\big( f'(Z^{(L+1)}) \\circ \\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}} \\big).\n",
    "\\tag{Equation 3}\n",
    "$$\n",
    "\n",
    "The derivation of these three equations is presented in the following three subsections.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  1.6.2.1 Step 1. Calculating the Partial Derivative of $C(W, B, X, y)$ with respect to the Weights\n",
    "\n",
    "Write $C(W, B, X, y)$ in terms of $a_j^{(L+1)}$ and since $a_j^{(L+1)} = f\\big(b_j^{(L)} + \\sum_{k = 0}^{n_L - 1} a_k^{(L)} w_{j k}^{(L)}\\big)$, then by the chain rule it follows that\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial w_{j k}^{(L)}}\n",
    "=\n",
    "\\frac{\\partial a_j^{(L+1)}}{\\partial w_{j k}^{(L)}} \\cdot \\frac{\\partial C(W, B, X, y)}{\\partial a_j^{(L+1)}}\n",
    "=\n",
    "\\frac{\\partial z_j^{(L+1)}}{\\partial w_{j k}^{(L)}} \\cdot \\frac{\\partial a_j^{(L+1)}}{\\partial z_j^{(L+1)}} \\cdot \\frac{\\partial C(W, B, X, y)}{\\partial a_j^{(L+1)}}\n",
    "$$\n",
    "for all $j$, $k$ and $L$.\n",
    "Substituting in \n",
    "$$\n",
    "\\frac{\\partial a_{j}^{(L+1)}}{\\partial z_{j}^{(L+1)}} = f'(z_{j}^{(L+1)})\n",
    "\\hspace{10pt} and \\hspace{10pt}\n",
    "\\frac{\\partial z_{j}^{(L+1)}}{\\partial w_{j k}^{(L)}} = a_k^{(L)}\n",
    "$$\n",
    "and rearranging gives\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial w_{j k}^{(L)}}\n",
    "=\n",
    "a_k^{(L)} f'(z_{j}^{(L+1)}) \\frac{\\partial C(W, B, X, y)}{\\partial a_j^{(L+1)}}.\n",
    "$$\n",
    "\n",
    "Substituting $\\frac{\\partial C(W, B, X, y)}{\\partial w_{j k}^{(L)}}$ into $\\frac{\\partial C(W, B, X, y)}{\\partial W^{(L)}}$ for all $j$ and $k$ gives the equation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial W^{(L)}}\n",
    "&=\n",
    "\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "f'(z_{0})^{(L+1)} \\\\\n",
    "f'(z_{1})^{(L+1)} \\\\\n",
    "f'(z_{2})^{(L+1)} \\\\\n",
    "\\vdots \\\\\n",
    "f'(z_{n_{L+1}})^{(L+1)}\n",
    "\\end{bmatrix}\n",
    "\\circ\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{0}^{(L+1)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{1}^{(L+1)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{2}^{(L+1)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{n_{L+1}}^{(L+1)}}\n",
    "\\end{bmatrix}\n",
    "\\Bigg) \n",
    "\\begin{bmatrix}\n",
    "a_{0}^{(L)} &\n",
    "a_{1}^{(L)} & \n",
    "a_{2}^{(L)} &\n",
    "\\dots &\n",
    "a_{n_L}^{(L)}\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "f'(z_{0})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{0}^{(L+1)}} \\\\\n",
    "f'(z_{1})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{1}^{(L+1)}} \\\\\n",
    "f'(z_{2})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{2}^{(L+1)}} \\\\\n",
    "\\vdots \\\\\n",
    "f'(z_{n_{L+1}})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{n_{L+1}}^{(L+1)}}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a_{0}^{(L)} \\\\\n",
    "a_{1}^{(L)} \\\\\n",
    "a_{2}^{(L)} \\\\\n",
    "\\dots \\\\\n",
    "a_{n_L}^{(L)}\n",
    "\\end{bmatrix}^T \\\\\n",
    "&=\n",
    "\\big( f'(Z^{(L+1)}) \\circ \\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}} \\big) (A^{(L)})^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\circ$ denotes Hadamard or element-wise multiplication (not matrix multiplication).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.6.2.2 Step 2. Calculating the Partial Derivative of $C(W, B, X, y)$ with respect to the Biases\n",
    "\n",
    "Write $C(W, B, X, y)$ in terms of $a_j^{(L+1)}$ and since $a_j^{(L+1)} = f\\big(b_j^{(L)} + \\sum_{k = 0}^{n_L - 1} a_k^{(L)} w_{j k}^{(L)}\\big)$, then by the chain rule it follows that\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial b_j^{(L)}}\n",
    "=\n",
    "\\frac{\\partial a_j^{(L+1)}}{\\partial b_j^{(L)}} \\cdot \\frac{\\partial C(W, B, X, y)}{\\partial a_j^{(L+1)}}\n",
    "=\n",
    "\\frac{\\partial z_j^{(L+1)}}{\\partial b_j^{(L)}} \\cdot \\frac{\\partial a_j^{(L+1)}}{\\partial z_j^{(L+1)}} \\cdot \\frac{\\partial C(W, B, X, y)}{\\partial a_j^{(L+1)}}\n",
    "$$\n",
    "for all $j$, $k$ and $L$.\n",
    "Substituting in \n",
    "$$\n",
    "\\frac{\\partial a_{j}^{(L+1)}}{\\partial z_{j}^{(L+1)}} = f'(z_{j}^{(L+1)})\n",
    "\\hspace{10pt} and \\hspace{10pt}\n",
    "\\frac{\\partial z_{j}^{(L+1)}}{\\partial b_j^{(L)}} = 1\n",
    "$$\n",
    "and rearranging gives\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial b_j^{(L)}}\n",
    "=\n",
    "f'(z_{j}^{(L+1)}) \\frac{\\partial C(W, B, X, y)}{\\partial a_j^{(L+1)}}.\n",
    "$$\n",
    "\n",
    "Substituting $\\frac{\\partial C(W, B, X, y)}{\\partial b_{j}^{(L)}}$ into $\\frac{\\partial C(W, B, X, y)}{\\partial B^{(L)}}$ for all $j$ gives the  equation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial B^{(L)}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "f'(z_{0})^{(L+1)} \\\\\n",
    "f'(z_{1})^{(L+1)} \\\\\n",
    "f'(z_{2})^{(L+1)} \\\\\n",
    "\\vdots \\\\\n",
    "f'(z_{n_{L+1}})^{(L+1)}\n",
    "\\end{bmatrix}\n",
    "\\circ\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{0}^{(L+1)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{1}^{(L+1)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{2}^{(L+1)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{n_{L+1}}^{(L+1)}}\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "f'(z_{0})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{0}^{(L+1)}} \\\\\n",
    "f'(z_{1})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{1}^{(L+1)}} \\\\\n",
    "f'(z_{2})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{2}^{(L+1)}} \\\\\n",
    "\\vdots \\\\\n",
    "f'(z_{n_{L+1}})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{n_{L+1}}^{(L+1)}}\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "f'(Z^{(L+1)}) \\circ \\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.6.2.3 Step 3. Calculating the Partial Derivative of  $C(W, B, X, y)$ with respect to the Activations\n",
    "\n",
    "Write $C(W, B, X, y)$ in terms of $a_j^{(L+1)}$ and since $a_j^{(L+1)} = f\\big(b_j^{(L)} + \\sum_{k = 0}^{n_L - 1} a_k^{(L)} w_{j k}^{(L)}\\big)$, then by the multivariable chain rule (see Section 0.2.7) it follows that\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{k}^{(L)}} \n",
    "= \n",
    "\\sum_{j=0}^{n_{L+1}-1} \\frac{\\partial a_{j}^{(L+1)}}{\\partial a_{k}^{(L)}} \\cdot \\frac{\\partial C(W, B, X, y)}{\\partial a_{j}^{(L+1)}}\n",
    "$$\n",
    "for all $k$ and by the chain rule it follows that\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{k}^{(L)}} \n",
    "=\n",
    "\\sum_{j=0}^{n_{L+1}-1} \\frac{\\partial z_{j}^{(L+1)}}{\\partial a_{k}^{(L)}} \\cdot \\frac{\\partial a_{j}^{(L+1)}}{\\partial z_{j}^{(L+1)}} \\cdot \\frac{\\partial C(W, B, X, y)}{\\partial a_{j}^{(L+1)}}\n",
    "$$\n",
    "for all $k$.  \n",
    "Substituting in \n",
    "$$\n",
    "\\frac{\\partial a_{j}^{(L+1)}}{\\partial z_{j}^{(L+1)}} = f'(z_{j}^{(L+1)})\n",
    "\\hspace{10pt} and \\hspace{10pt}\n",
    "\\frac{\\partial z_{j}^{(L+1)}}{\\partial a_{k}^{(L)}} = w_{j k}^{(L)}\n",
    "$$\n",
    "and rearranging gives\n",
    "$$\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{k}^{(L)}} \n",
    "=\n",
    "\\sum_{j=0}^{n_{L+1}-1} w_{j k}^{(L)} f'(z_{j}^{(L+1)}) \\frac{\\partial C(W, B, X, y)}{\\partial a_{j}^{(L+1)}};\n",
    "$$\n",
    "that is, $\\frac{\\partial C(W, B, X, y)}{\\partial a_{k}^{(L)}}$ is the dot product\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{k}^{(L)}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "w_{0 k}^{(L)} \\\\\n",
    "w_{1 k}^{(L)} \\\\\n",
    "w_{2 k}^{(L)} \\\\\n",
    "\\dots \\\\ \n",
    "w_{n_{L+1}-1 k}^{(L)}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "f'(z_{0})^{(L+1)} \\\\\n",
    "f'(z_{1})^{(L+1)} \\\\\n",
    "f'(z_{2})^{(L+1)} \\\\\n",
    "\\vdots \\\\\n",
    "f'(z_{n_{L+1}-1})^{(L+1)}\n",
    "\\end{bmatrix}\n",
    "\\circ\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{0}^{(L+1)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{1}^{(L+1)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{2}^{(L+1)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{n_{L+1}-1}^{(L+1)}}\n",
    "\\end{bmatrix}\n",
    "\\Bigg) \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "w_{0 k}^{(L)} \\\\\n",
    "w_{1 k}^{(L)} \\\\\n",
    "w_{2 k}^{(L)} \\\\\n",
    "\\ldots \\\\\n",
    "w_{n_{L+1}-1 k}^{(L)}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "f'(z_{0})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{0}^{(L+1)}} \\\\\n",
    "f'(z_{1})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{1}^{(L+1)}} \\\\\n",
    "f'(z_{2})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{2}^{(L+1)}} \\\\\n",
    "\\vdots \\\\\n",
    "f'(z_{n_{L+1}-1})^{(L+1)} \\frac{\\partial C(W, B, X, y)}{\\partial a_{n_{L+1}-1}^{(L+1)}}\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "w_{0 k}^{(L)} \\\\\n",
    "w_{1 k}^{(L)} \\\\\n",
    "w_{2 k}^{(L)} \\\\\n",
    "\\ldots \\\\\n",
    "w_{n_{L+1}-1 k}^{(L)}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\big( f'(Z^{(L+1)}) \\circ \\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}} \\big) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\cdot$ denotes dot product (not matrix multiplication).\n",
    "\n",
    "Substituting $\\frac{\\partial C(W, B, X, y)}{\\partial a_{k}^{(L)}}$ into $\\frac{\\partial C(W, B, X, y)}{\\partial A^{(L)}}$ for all $k$ gives the equation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial A^{(L)}}\n",
    "&=\n",
    "\\begin{pmatrix}\n",
    "w_{0 0}^{(L)} & w_{1 0}^{(L)} & w_{2 0}^{(L)} \\dots w_{n_{L+1}-1 0}^{(L)} \\\\\n",
    "w_{0 1}^{(L)} & w_{1 1}^{(L)} & w_{2 1}^{(L)} \\dots w_{n_{L+1}-1 1}^{(L)} \\\\\n",
    "w_{0 2}^{(L)} & w_{1 2}^{(L)} & w_{2 2}^{(L)} \\dots w_{n_{L+1}-1 2}^{(L)} \\\\\n",
    "\\vdots \\\\\n",
    "w_{0 n_L - 1}^{(L)} & w_{1 n_L - 1}^{(L)} & w_{2 n_L - 1}^{(L)} \\dots w_{n_{L+1}-1 n_L-1}^{(L)}\n",
    "\\end{pmatrix}\n",
    "\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "f'(z_{0})^{(L+1)} \\\\\n",
    "f'(z_{1})^{(L+1)} \\\\\n",
    "f'(z_{2})^{(L+1)} \\\\\n",
    "\\vdots \\\\\n",
    "f'(z_{n_{L+1}-1})^{(L+1)}\n",
    "\\end{bmatrix}\n",
    "\\circ\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{0}^{(L+1)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{1}^{(L+1)}} \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{2}^{(L+1)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial C(W, B, X, y)}{\\partial a_{n_{L+1}-1}^{(L+1)}}\n",
    "\\end{bmatrix}\n",
    "\\Bigg) \\\\\n",
    "&= (W^{(L)})^T \\big( f'(Z^{(L+1)}) \\circ \\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}} \\big).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Image 4\n",
    "\n",
    "Information flows through an artificial neuron from left to right, so laying the terms of \n",
    "$$\n",
    "f'(Z^{(L+1)}) \\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}}\n",
    "$$\n",
    "over the top of a diagram of an artificial neuron in place of terms of $A^{(L+1)}$, the fact that $\\frac{\\partial C(W, B, X, y)}{\\partial A^{(L+1)}}$ propagates from right to left, or backwards, is visualisable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NNBack2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 Notation\n",
    "\n",
    "In some texts the notation $\\delta_i^{(L)} = f'(z_i^{(L)}) \\frac{\\partial C(W,B,X,y)}{\\partial a_i^{(L)}}$ is used and it is the values of $\\delta_i^{(L)}$ rather than the values of $\\frac{\\partial C(W,B,X,y)}{\\partial a_i^{(L)}}$ that are said to propagate backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.4 The Derivative of the Activation Functions\n",
    "\n",
    "For a function $f(x)$ its derivative with respect to $x$ is denoted as $f'(x)$ and sometimes as $\\frac{dy}{dx}$ or $\\frac{df(x)}{dx}$.\n",
    "\n",
    "The derivative of the rectified linear unit is \n",
    "$$\n",
    "\\rm{ReLU}'(x) = \n",
    "\\begin{cases}\n",
    "0 & x \\leq 0 \\\\\n",
    "1 & x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "The derivative of the sigmoid function is\n",
    "$$\n",
    "\\sigma'(x) = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\sigma(x)(1-\\sigma(x)).\n",
    "$$\n",
    "The derivative of the hyperbolic tangent function is \n",
    "$$\n",
    "\\rm{tanh}'(x) = \\frac{4}{(1 + e^{-1})^2} = 1 - \\rm{tanh}(x)^2.\n",
    "$$\n",
    "\n",
    "These functions are written as follows in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_der(x):\n",
    "    max_0x = np.maximum(0,x)\n",
    "    with np.nditer(max_0x, op_flags=['readwrite']) as it:\n",
    "        for u in it:\n",
    "            if u > 0:\n",
    "                u[...] = 1.0\n",
    "    return max_0x\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def tanh_der(x):\n",
    "    return 1-(tanh(x)*tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.5 Calculating the Partial Derivative of $C(W, B)$ for all Weights and Biases\n",
    "\n",
    "Finally, with everything calculated apply the update for batch gradient descent as follows.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W, B)}{\\partial w_{j k}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}\n",
    "= \n",
    "\\frac{1}{m} \\sum_{\\forall (X, y) \\in S} \\frac{\\partial C(W, B, X, y)}{\\partial w_{j k}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}\n",
    "=\n",
    "\\frac{1}{m} \\sum_{\\forall (X, y) \\in S} a_k^{(L)} f'(z_{j}^{(L+1)}) \\frac{\\partial C(W, B, X, y)}{\\partial a_j^{(L+1)}} \\Bigg|_{\\bar{W}, \\bar{B}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial C(W, B)}{\\partial b_{j}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}\n",
    "= \n",
    "\\frac{1}{m} \\sum_{\\forall (X, y) \\in S} \\frac{\\partial C(W, B, X, y)}{\\partial b_{j}^{(L)}} \\Bigg|_{\\bar{W}, \\bar{B}}\n",
    "=\n",
    "\\frac{1}{m} \\sum_{\\forall (X, y) \\in S} f'(z_{j}^{(L+1)}) \\frac{\\partial C(W, B, X, y)}{\\partial a_j^{(L+1)}} \\Bigg|_{\\bar{W}, \\bar{B}}.\n",
    "$$\n",
    "\n",
    "Similarly for stochastic gradient descent and mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Representation of Gradient Descent with Backward Propagation in Python\n",
    "\n",
    "- Class *network_with_backdrop* - Extension of *network*.\n",
    " - Method *backdrop* - Implements the backward propagation algorithm.  \n",
    " Input: $Z$ and $A$ that are the output of *forward_pass* and the target vector $y$.  \n",
    " Output: $D\\_W$, $D\\_B$, $\\text{mean}\\_D\\_W$ and $\\text{mean}\\_D\\_B$  \n",
    " where $D\\_W$ is list with elements \n",
    " $\\Bigg( \\frac{\\partial C(W,B,X,y)}{\\partial W^{(L)}} \\Bigg)^T$ for all $L = 0, 1, \\ldots, N-2$ \n",
    " represented as a NumPy array with shape $(s, n_L, n_{L+1})$  \n",
    " and $D\\_B$ is list with elements \n",
    " $\\Bigg( \\frac{\\partial C(W,B,X,y)}{\\partial B^{(L)}} \\Bigg)^T$ for all $L = 0, 1, \\ldots, N-2$ \n",
    " represented as a NumPy array with shape $(s, 1, n_{L+1})$\n",
    " and $\\text{mean}\\_D\\_W$ and $\\text{mean}\\_D\\_B$ are their means.\n",
    " - Method *update* - Updates the weights and biases.\n",
    "\n",
    "As $A^{(L)}$ and other column vectors will be represented by row vectors in NumPy, the transposed versions of Equations 1, 2 and 3 are used in place of the original equations, that is, the equations\n",
    "\n",
    "$$\n",
    "\\Bigg( \\frac{\\partial C(W,B,X,y)}{\\partial W^{(L)}} \\Bigg)^T = A^{(L)} \\big( f'((Z^{(L+1)})^T) \\circ \\Bigg( \\frac{\\partial C(W,B,X,y)}{\\partial A^{(L+1)}} \\Bigg)^T \\big), \\tag{Equation 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Bigg( \\frac{\\partial C(W,B,X,y)}{\\partial B^{(L)}} \\Bigg)^T = f'((Z^{(L+1)})^T) \\circ \\Bigg( \\frac{\\partial C(W,B,X,y)}{\\partial A^{(L+1)}} \\Bigg)^T, \\tag{Equation 2}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\Bigg( \\frac{\\partial C(W,B,X,y)}{\\partial A^{(L)}} \\Bigg)^T = \\big( f'((Z^{(L+1)})^T) \\circ \\Bigg( \\frac{\\partial C(W,B,X,y)}{\\partial A^{(L+1)}} \\Bigg)^T \\big) W^{(L)}, \\tag{Equation 3}\n",
    "$$\n",
    "\n",
    "are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.d. = partial derivative\n",
    "# w.r.t. = with respect to\n",
    "\n",
    "class network_with_backprop(network):\n",
    "    def __init__(self):\n",
    "        self._nodes_per_layer = []\n",
    "        self._all_connections = []\n",
    "    \n",
    "    def backprop(self, Z, A, y):\n",
    "        D_A_L_1 = A[-1] - y # p.d. of C(W, B, X, y) w.r.t. A^{(L+1)}.\n",
    "        \n",
    "        D_W = []\n",
    "        D_B = []\n",
    "        mean_D_W = []\n",
    "        mean_D_B = []\n",
    "        \n",
    "        bs = A[0].shape[0] # batch size\n",
    "        \n",
    "        for i in range(len(A)-1,0,-1):\n",
    "            nodes_L = self._nodes_per_layer[i-1] # Number of nodes in the L-th Layer\n",
    "            nodes_L_1 = self._nodes_per_layer[i] # Number of nodes in the (L+1)-th layer\n",
    "            \n",
    "            # Equation 1\n",
    "            # p.d. of C(W, B, X, y) w.r.t. W^{(L)}\n",
    "            D_W_L = np.matmul( A[i-1].reshape(bs,nodes_L,1), (sigmoid_der(Z[i])*D_A_L_1).reshape(bs,1,nodes_L_1) )\n",
    "            \n",
    "            # Equation 2\n",
    "            # p.d. of C(W, B, X, y) w.r.t. B^{(L)}\n",
    "            D_B_L = sigmoid_der(Z[i])*D_A_L_1\n",
    "            \n",
    "            # Equation 3\n",
    "            # p.d. of C(W, B, X, y) w.r.t. A^{(L)}\n",
    "            D_A_L = np.matmul( sigmoid_der(Z[i])*D_A_L_1, np.transpose(self._all_connections[i-1].W) )\n",
    "            \n",
    "            D_W.insert(0, D_W_L)\n",
    "            \n",
    "            # The average of D_W over all batches, that is, axis 0\n",
    "            mean_D_W_L = np.sum((1/bs)*D_W_L, axis = 0)\n",
    "            mean_D_W.insert(0, mean_D_W_L)\n",
    "            \n",
    "            D_B.insert(0, D_B_L)\n",
    "            \n",
    "            # The average of D_B over all batches, that is, axis 0\n",
    "            mean_D_B_L = np.sum((1/bs)*D_B_L, axis = 0)\n",
    "            mean_D_B.insert(0, mean_D_B_L.reshape(1, mean_D_B_L.shape[0])) # want shape (1,n) not (n,)\n",
    "            \n",
    "            D_A_L_1 = D_A_L\n",
    "                    \n",
    "        return D_W, D_B, mean_D_W, mean_D_B\n",
    "    \n",
    "    def update(self, mean_D_W, mean_D_B, learning_rate):\n",
    "        for layer in range(len(self._all_connections)):\n",
    "            # Update the weights\n",
    "            self._all_connections[layer].W = self._all_connections[layer].W - learning_rate*mean_D_W[layer]\n",
    "            # Update the biases\n",
    "            self._all_connections[layer].B = self._all_connections[layer].B - learning_rate*mean_D_B[layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Worked Example\n",
    "\n",
    "In this example SciKit-Learn's *digits* set is used to train and test neural networks created with the *network* and *network_with_backdrop* classes using batch gradient descent, stochastic gradient descent and mini-batch gradient descent.\n",
    "\n",
    "Digits consists of $1797$ vectors of $64$ elements and a corresponding label consisting of a digit between $0$ and $9$.\n",
    "The vectors represent an $8$ by $8$ black and white image of a hand-drawn digit corresponding to the label.\n",
    "\n",
    "- Input vector - a vector of 64 elements scaled over all samples\n",
    "- Output vector - a vector of 10 elements (the one-hot encoded representation of the corresponding digit)\n",
    "\n",
    "SciKit-Learn has a built-in class for achieving Scaling called *StandardScaler*.\n",
    "If $n \\in [0,9]$, its one-hot transformation is a vector of length $10$ with a $1$ in position $n$ and $0$ elsewhere.\n",
    "SciKit-Learn has a built-in function *OneHotEncoder* to perform this transformation.\n",
    "\n",
    "With all data in an appropriate form, the neural networks are created and trained; the aim of the training is to accurately predict which digit is drawn in the $8$ by $8$ square.\n",
    "\n",
    "For testing accuracy SciKit-Learn has a built-in function *accuracy_score* that is utilised.\n",
    "\n",
    "#### Setup\n",
    "- Two hidden layers with 256 artificial neurons.\n",
    "- $\\eta = 0.1$\n",
    "- $3000$ epochs\n",
    "- Mini-batch size of $64$\n",
    "\n",
    "#### Result\n",
    "- Batch Gradient Descent achieved 65.83% accuracy.\n",
    "- Stochastic Gradient Descent achieved 97.22% accuracy.\n",
    "- Mini-Batch Gradient Descent achieved 95.28% accuracy.\n",
    "\n",
    "#### Analysis\n",
    "While Stochastic gradient descent is the most accurate it is near its maximum level, however the accuracy for batch gradient descent increases significantly if $\\eta = 0.25$ and continues to increase if the number of epochs is increased, because it is slower to converge.\n",
    "With mini-batch gradient descent changing the batch size to $128$ decreased accuracy significantly over $3000$ epochs, on one run to below $60%$.\n",
    "I recommend that the reader plays with the hyperparameters and changes between ReLU, sigmoid and tanh, to see how the results differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
      "       ...,\n",
      "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
      "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
      "       [ 0.,  0., 10., ..., 12.,  1.,  0.]]), 'target': array([0, 1, 2, ..., 8, 9, 8]), 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
      "        [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
      "        [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
      "        [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
      "        [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
      "        [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
      "        [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
      "        [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
      "        [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
      "        [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
      "        [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
      "        [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
      "        [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
      "        [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
      "        [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
      "        [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]), 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\"}\n"
     ]
    }
   ],
   "source": [
    "# Import data from SciKit-Learn\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "# Take a look at digits\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit.data.shape =  (1797, 64)\n",
      "digit.target.shape =  (1797,)\n",
      "digit.target_names =  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the structure of digits\n",
    "print(\"digit.data.shape = \", digits.data.shape)\n",
    "print(\"digit.target.shape = \", digits.target.shape)\n",
    "print(\"digit.target_names = \", digits.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data =  [ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "target =  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAL40lEQVR4nO3dW4hd9RXH8d+vY7xGSaxWJBHtSAmIUHNBKgFpNYpWsS81RFCotCQPrRha0NiX4ptPYh+KELxU8IajBoq01gQVEVrtTIz1MrFoiJhEHSWRGAsR4+rD2SkxnTp7xv3/z5mzvh845MzMmb3WzOR39t7n7L2XI0IABtu3ZrsBAOURdCABgg4kQNCBBAg6kABBBxLoi6DbvsL2W7bftr2hcK37bE/Yfr1knSPqnWX7Odvjtt+wfXPhesfbftn2q02920vWa2oO2X7F9lOlazX1dtp+zfY226OFay2w/bjt7c3f8KKCtZY0P9Ph237b6ztZeETM6k3SkKR3JA1LOlbSq5LOK1jvYknLJL1e6ec7U9Ky5v7Jkv5V+OezpPnN/XmSXpL0g8I/468lPSzpqUq/052STqtU6wFJv2juHytpQaW6Q5I+kHR2F8vrhzX6hZLejogdEfG5pEcl/aRUsYh4QdLeUsufpN77EbG1uf+ppHFJiwrWi4g40Hw4r7kVOyrK9mJJV0m6p1SN2WL7FPVWDPdKUkR8HhGfVCp/qaR3IuLdLhbWD0FfJOm9Iz7epYJBmE22z5G0VL21bMk6Q7a3SZqQtDkiSta7S9Itkr4sWONoIekZ22O21xasMyzpI0n3N7sm99g+qWC9I62R9EhXC+uHoHuSzw3ccbm250t6QtL6iNhfslZEHIqICyQtlnSh7fNL1LF9taSJiBgrsfyvsTIilkm6UtIvbV9cqM4x6u3m3R0RSyV9Jqnoa0iSZPtYSddIGulqmf0Q9F2Szjri48WS9sxSL0XYnqdeyB+KiCdr1W02M5+XdEWhEislXWN7p3q7XJfYfrBQrf+KiD3NvxOSNqm3+1fCLkm7jtgiely94Jd2paStEfFhVwvsh6D/Q9L3bH+3eSZbI+lPs9xTZ2xbvX288Yi4s0K9020vaO6fIGmVpO0lakXEbRGxOCLOUe/v9mxEXF+i1mG2T7J98uH7ki6XVOQdlIj4QNJ7tpc0n7pU0pslah3lOnW42S71Nk1mVUR8YftXkv6q3iuN90XEG6Xq2X5E0g8lnWZ7l6TfRcS9peqpt9a7QdJrzX6zJP02Iv5cqN6Zkh6wPaTeE/ljEVHlba9KzpC0qff8qWMkPRwRTxesd5Okh5qV0A5JNxasJdsnSrpM0rpOl9u8lA9ggPXDpjuAwgg6kABBBxIg6EACBB1IoK+CXvhwxlmrRT3qzXa9vgq6pJq/zKp/OOpRbzbr9VvQARRQ5IAZ2wN9FM7ChQun/T0HDx7UcccdN6N6ixZN/2S+vXv36tRTT51Rvf37p3/OzYEDBzR//vwZ1du9e/e0vyci1BwdN22HDh2a0ffNFRHxP7+YWT8Edi5atWpV1Xp33HFH1XpbtmypWm/DhuInhH3Fvn37qtbrB2y6AwkQdCABgg4kQNCBBAg6kABBBxIg6EACBB1IoFXQa45MAtC9KYPeXGTwD+pdgvY8SdfZPq90YwC602aNXnVkEoDutQl6mpFJwKBqc1JLq5FJzYnytc/ZBdBCm6C3GpkUERslbZQG/zRVYK5ps+k+0COTgAymXKPXHpkEoHutLjzRzAkrNSsMQGEcGQckQNCBBAg6kABBBxIg6EACBB1IgKADCRB0IAEmtcxA7ckpw8PDVevNZOTUN7F3796q9VavXl213sjISNV6k2GNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQTajGS6z/aE7ddrNASge23W6H+UdEXhPgAUNGXQI+IFSXXPOgDQKfbRgQQ6O02V2WtA/+os6MxeA/oXm+5AAm3eXntE0t8kLbG9y/bPy7cFoEtthixeV6MRAOWw6Q4kQNCBBAg6kABBBxIg6EACBB1IgKADCRB0IIGBmL22fPnyqvVqz0I799xzq9bbsWNH1XqbN2+uWq/2/xdmrwGogqADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJtLk45Fm2n7M9bvsN2zfXaAxAd9oc6/6FpN9ExFbbJ0sas705It4s3BuAjrSZvfZ+RGxt7n8qaVzSotKNAejOtPbRbZ8jaamkl0o0A6CM1qep2p4v6QlJ6yNi/yRfZ/Ya0KdaBd32PPVC/lBEPDnZY5i9BvSvNq+6W9K9ksYj4s7yLQHoWpt99JWSbpB0ie1tze3HhfsC0KE2s9delOQKvQAohCPjgAQIOpAAQQcSIOhAAgQdSICgAwkQdCABgg4kMBCz1xYuXFi13tjYWNV6tWeh1Vb795kRa3QgAYIOJEDQgQQIOpAAQQcSIOhAAgQdSICgAwkQdCABgg4k0OYqsMfbftn2q83stdtrNAagO22OdT8o6ZKIONBc3/1F23+JiL8X7g1AR9pcBTYkHWg+nNfcGNAAzCGt9tFtD9neJmlC0uaIYPYaMIe0CnpEHIqICyQtlnSh7fOPfozttbZHbY923SSAb2Zar7pHxCeSnpd0xSRf2xgRKyJiRUe9AehIm1fdT7e9oLl/gqRVkraXbgxAd9q86n6mpAdsD6n3xPBYRDxVti0AXWrzqvs/JS2t0AuAQjgyDkiAoAMJEHQgAYIOJEDQgQQIOpAAQQcSIOhAAsxem4EtW7ZUrTfoav/99u3bV7VeP2CNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQRaB70Z4vCKbS4MCcwx01mj3yxpvFQjAMppO5JpsaSrJN1Tth0AJbRdo98l6RZJXxbsBUAhbSa1XC1pIiLGpngcs9eAPtVmjb5S0jW2d0p6VNIlth88+kHMXgP615RBj4jbImJxRJwjaY2kZyPi+uKdAegM76MDCUzrUlIR8bx6Y5MBzCGs0YEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJDAQs9dqz9Javnx51Xq11Z6FVvv3OTIyUrVeP2CNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQRaHQLbXOr5U0mHJH3BJZ2BuWU6x7r/KCI+LtYJgGLYdAcSaBv0kPSM7THba0s2BKB7bTfdV0bEHtvfkbTZ9vaIeOHIBzRPADwJAH2o1Ro9IvY0/05I2iTpwkkew+w1oE+1maZ6ku2TD9+XdLmk10s3BqA7bTbdz5C0yfbhxz8cEU8X7QpAp6YMekTskPT9Cr0AKIS314AECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJOCI6H6hdvcL/RrDw8M1y2l0dLRqvXXr1lWtd+2111atV/vvt2LFYJ+OERE++nOs0YEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQQIOpBAq6DbXmD7cdvbbY/bvqh0YwC603aAw+8lPR0RP7V9rKQTC/YEoGNTBt32KZIulvQzSYqIzyV9XrYtAF1qs+k+LOkjSffbfsX2Pc0gh6+wvdb2qO26p3YBmFKboB8jaZmkuyNiqaTPJG04+kGMZAL6V5ug75K0KyJeaj5+XL3gA5gjpgx6RHwg6T3bS5pPXSrpzaJdAehU21fdb5L0UPOK+w5JN5ZrCUDXWgU9IrZJYt8bmKM4Mg5IgKADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIDMXuttrVr11atd+utt1atNzY2VrXe6tWrq9YbdMxeA5Ii6EACBB1IgKADCRB0IAGCDiRA0IEECDqQAEEHEpgy6LaX2N52xG2/7fU1mgPQjSmvGRcRb0m6QJJsD0naLWlT4b4AdGi6m+6XSnonIt4t0QyAMqYb9DWSHinRCIByWge9uab7NZJG/s/Xmb0G9Km2Axwk6UpJWyPiw8m+GBEbJW2UBv80VWCumc6m+3Visx2Yk1oF3faJki6T9GTZdgCU0HYk078lfbtwLwAK4cg4IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQggVKz1z6SNJNz1k+T9HHH7fRDLepRr1a9syPi9KM/WSToM2V7NCJWDFot6lFvtuux6Q4kQNCBBPot6BsHtBb1qDer9fpqHx1AGf22RgdQAEEHEiDoQAIEHUiAoAMJ/AchD47vPuZI8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a look at an example\n",
    "print(\"data = \", digits.data[0])\n",
    "print(\"target = \", digits.target[0])\n",
    "\n",
    "# Display a sample image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "[ 0.         -0.33501649 -0.04308102  0.27407152 -0.66447751 -0.84412939\n",
      " -0.40972392 -0.12502292 -0.05907756 -0.62400926  0.4829745   0.75962245\n",
      " -0.05842586  1.12772113  0.87958306 -0.13043338 -0.04462507  0.11144272\n",
      "  0.89588044 -0.86066632 -1.14964846  0.51547187  1.90596347 -0.11422184\n",
      " -0.03337973  0.48648928  0.46988512 -1.49990136 -1.61406277  0.07639777\n",
      "  1.54181413 -0.04723238  0.          0.76465553  0.05263019 -1.44763006\n",
      " -1.73666443  0.04361588  1.43955804  0.         -0.06134367  0.8105536\n",
      "  0.63011714 -1.12245711 -1.06623158  0.66096475  0.81845076 -0.08874162\n",
      " -0.03543326  0.74211893  1.15065212 -0.86867056  0.11012973  0.53761116\n",
      " -0.75743581 -0.20978513 -0.02359646 -0.29908135  0.08671869  0.20829258\n",
      " -0.36677122 -1.14664746 -0.5056698  -0.19600752]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the input data\n",
    "# Scale digits.data with SciKit-Learn's StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(digits.data)\n",
    "digits.data = scaler.transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(digits.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SciKit-Learn's built-in function train_test_split to get training and and test data with 80-20 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# One-Hot encode target training data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(categories=[digits.target_names])\n",
    "y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train).toarray()\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick check to ensure that values have not vanished or exploded\n",
    "def diverges(lis):\n",
    "    for i in range(len(lis)):\n",
    "        if np.isnan(lis[i]).any() or np.isinf(lis[i]).any():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  0\n",
      "Epoch =  100\n",
      "Epoch =  200\n",
      "Epoch =  300\n",
      "Epoch =  400\n",
      "Epoch =  500\n",
      "Epoch =  600\n",
      "Epoch =  700\n",
      "Epoch =  800\n",
      "Epoch =  900\n",
      "Epoch =  1000\n",
      "Epoch =  1100\n",
      "Epoch =  1200\n",
      "Epoch =  1300\n",
      "Epoch =  1400\n",
      "Epoch =  1500\n",
      "Epoch =  1600\n",
      "Epoch =  1700\n",
      "Epoch =  1800\n",
      "Epoch =  1900\n",
      "Epoch =  2000\n",
      "Epoch =  2100\n",
      "Epoch =  2200\n",
      "Epoch =  2300\n",
      "Epoch =  2400\n",
      "Epoch =  2500\n",
      "Epoch =  2600\n",
      "Epoch =  2700\n",
      "Epoch =  2800\n",
      "Epoch =  2900\n"
     ]
    }
   ],
   "source": [
    "# Batch Gradient Descent\n",
    "\n",
    "# Create Network\n",
    "digits_network = network_with_backprop()\n",
    "\n",
    "# Add layer\n",
    "digits_network.add_layer(64)\n",
    "digits_network.add_layer(256)\n",
    "digits_network.add_layer(256)\n",
    "digits_network.add_layer(10)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "number_of_epochs = 3000\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print(\"Epoch = \", epoch)\n",
    "    \n",
    "    Z, A = digits_network.forward_pass(X_train)\n",
    "    if diverges(Z) or diverges(A):\n",
    "        print(\"Diverges!\")\n",
    "        break\n",
    "    \n",
    "    D_W, D_B, mean_D_W, mean_D_B = digits_network.backprop(Z, A, y_train)\n",
    "    if diverges(mean_D_W) or diverges(mean_D_B):\n",
    "        print(\"Diverges!\")\n",
    "        break\n",
    "    \n",
    "    digits_network.update(mean_D_W, mean_D_B, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.83333333333333"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Batch Gradient Descent\n",
    "\n",
    "# Pass the test data through the network\n",
    "Z, A = digits_network.forward_pass(X_test)\n",
    "\n",
    "# The predicted digit is the one with the largest output, that is, the one nearest to the shape 0,0,...,0,1,0,...,0,0\n",
    "y_pred = np.array([np.argmax(i) for i in A[-1]])\n",
    "\n",
    "# Use SciKit-learn's built-in function accuracy_score to find the accuracy of the network in percentage terms\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  0\n",
      "Epoch =  100\n",
      "Epoch =  200\n",
      "Epoch =  300\n",
      "Epoch =  400\n",
      "Epoch =  500\n",
      "Epoch =  600\n",
      "Epoch =  700\n",
      "Epoch =  800\n",
      "Epoch =  900\n",
      "Epoch =  1000\n",
      "Epoch =  1100\n",
      "Epoch =  1200\n",
      "Epoch =  1300\n",
      "Epoch =  1400\n",
      "Epoch =  1500\n",
      "Epoch =  1600\n",
      "Epoch =  1700\n",
      "Epoch =  1800\n",
      "Epoch =  1900\n",
      "Epoch =  2000\n",
      "Epoch =  2100\n",
      "Epoch =  2200\n",
      "Epoch =  2300\n",
      "Epoch =  2400\n",
      "Epoch =  2500\n",
      "Epoch =  2600\n",
      "Epoch =  2700\n",
      "Epoch =  2800\n",
      "Epoch =  2900\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "digits_network_stoch = network_with_backprop()\n",
    "\n",
    "digits_network_stoch.add_layer(64)\n",
    "digits_network_stoch.add_layer(256)\n",
    "digits_network_stoch.add_layer(256)\n",
    "digits_network_stoch.add_layer(10)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "number_of_epochs = 3000\n",
    "\n",
    "# Number of Training Samples\n",
    "m = len(X_train)\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print(\"Epoch = \", epoch)\n",
    "    \n",
    "    index = np.arange(m)\n",
    "    np.random.shuffle(index)\n",
    "    \n",
    "    for i in range(m):\n",
    "        X = X_train[index[i]]\n",
    "        y = y_train[index[i]]\n",
    "        X = np.array([X])\n",
    "        y = np.array([y])\n",
    "        \n",
    "        Z, A = digits_network_stoch.forward_pass(X)\n",
    "        if diverges(Z) or diverges(A):\n",
    "            print(\"Diverges!\")\n",
    "            break\n",
    "        \n",
    "        D_W, D_B, mean_D_W, mean_D_B = digits_network_stoch.backprop(Z, A, y)\n",
    "        if diverges(mean_D_W) or diverges(mean_D_B):\n",
    "            print(\"Diverges!\")\n",
    "            break\n",
    "        \n",
    "        digits_network_stoch.update(mean_D_W, mean_D_B, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.22222222222221"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Stochastic Gradient Descent\n",
    "\n",
    "# Pass the test data through the network\n",
    "Z_stoch, A_stoch = digits_network_stoch.forward_pass(X_test)\n",
    "\n",
    "# The predicted digit is the one with the largest output, that is, the one nearest to the shape 0,0,...,0,1,0,...,0,0\n",
    "y_pred_stoch = np.array([np.argmax(i) for i in A_stoch[-1]])\n",
    "\n",
    "# Use SciKit-learn's built-in function accuracy_score to find the accuracy of the network in percentage terms\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred_stoch)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  0\n",
      "Epoch =  100\n",
      "Epoch =  200\n",
      "Epoch =  300\n",
      "Epoch =  400\n",
      "Epoch =  500\n",
      "Epoch =  600\n",
      "Epoch =  700\n",
      "Epoch =  800\n",
      "Epoch =  900\n",
      "Epoch =  1000\n",
      "Epoch =  1100\n",
      "Epoch =  1200\n",
      "Epoch =  1300\n",
      "Epoch =  1400\n",
      "Epoch =  1500\n",
      "Epoch =  1600\n",
      "Epoch =  1700\n",
      "Epoch =  1800\n",
      "Epoch =  1900\n",
      "Epoch =  2000\n",
      "Epoch =  2100\n",
      "Epoch =  2200\n",
      "Epoch =  2300\n",
      "Epoch =  2400\n",
      "Epoch =  2500\n",
      "Epoch =  2600\n",
      "Epoch =  2700\n",
      "Epoch =  2800\n",
      "Epoch =  2900\n"
     ]
    }
   ],
   "source": [
    "# Mini-Batch Gradient Descent\n",
    "\n",
    "# Create Network\n",
    "digits_network_mini = network_with_backprop()\n",
    "\n",
    "digits_network_mini.add_layer(64)\n",
    "digits_network_mini.add_layer(256)\n",
    "digits_network_mini.add_layer(256)\n",
    "digits_network_mini.add_layer(10)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "number_of_epochs = 3000\n",
    "\n",
    "# Number of Training Samples\n",
    "m = len(X_train)\n",
    "\n",
    "# Mini-Batch Size\n",
    "bs = 64\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print(\"Epoch = \", epoch)\n",
    "    \n",
    "    index = np.arange(m)\n",
    "    np.random.shuffle(index)\n",
    "    \n",
    "    for i in range(int(m/bs)):\n",
    "        Sub_X = np.array([X_train[index[bs*i + j]] for j in range(bs)])\n",
    "        Sub_y = np.array([y_train[index[bs*i + j]] for j in range(bs)])\n",
    "        \n",
    "        Z, A = digits_network_mini.forward_pass(Sub_X)\n",
    "        if diverges(Z) or diverges(A):\n",
    "            print(\"Diverges!\")\n",
    "            break\n",
    "        \n",
    "        D_W, D_B, mean_D_W, mean_D_B = digits_network_mini.backprop(Z, A, Sub_y)\n",
    "        if diverges(mean_D_W) or diverges(mean_D_B):\n",
    "            print(\"Diverges!\")\n",
    "            break\n",
    "        \n",
    "        digits_network_mini.update(mean_D_W, mean_D_B, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.27777777777777"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Mini-Batch Gradient Descent\n",
    "\n",
    "# Pass the test data through the network\n",
    "Z_mini, A_mini = digits_network_mini.forward_pass(X_test)\n",
    "\n",
    "# The predicted digit is the one with the largest output, that is, the one nearest to the shape 0,0,...,0,1,0,...,0,0\n",
    "y_pred_mini = np.array([np.argmax(i) for i in A_mini[-1]])\n",
    "\n",
    "# Use SciKit-learn's built-in function accuracy_score to find the accuracy of the network in percentage terms\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred_mini)*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
